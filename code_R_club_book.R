# R code 2.1
ways <- c(0, 3, 8, 9, 0)
ways/sum(ways)
fomula(sum)

# R code 2.2
temp<- dbinom(6, size=9, prob = 0.5)
temp
help(dbinom)

# R code 2.3
# define grid
p_grid <- seq(from = 0, to = 1, length.out = 100) 
# p_grid <- seq(from = 0, to = 1, by=0.2)
p_grid
# define prior
prior <- rep(2, 100)
# prior <- exp(-5*abs(p_grid - 0.5))
help("ifelse")
# log(exp(3))
prior

# compute likelihood at each value in grid
likelihood <- dbinom(6, size = 9, prob = p_grid)
likelihood
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# R code 2.4, display the posterior distribution
plot(p_grid, posterior, type = "b", 
     xlab="probability of water", ylab="posterior probabilty")
mtext("20 points")
help(plot)

# R code 2.5: replicate the different prior in the previous code

# R code 0.5
install.packages(c("coda","mvtnorm","devtools"))
library(devtools)
devtools::install_github("rmcelreath/rethinking")

# R code 2.6 
library(rethinking)
globe.qa <- map(
  alist(
    w ~ dbinom(9,p), # binormial likelihood
    p ~ dunif(0,1)   # uniform prior
  ),
  data = list(w=6)
)
# display summary of quandratic approximation
precis(globe.qa)

help("~")
help("dbeta")
data = list(w=6)
data

# R code 2.7
# analytical calculation
w <- 6
n <- 9 
curve(dbeta(x, w+1, n-w+1), from = 0, to = 1)
# quandratic approximation
curve(dnorm(x, 0.67, 0.16), lty=2, add=TRUE)

# 2M1
# R code 2.3
# define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)
p_grid
# define prior
prior <- rep(1, 20)
prior
# compute likelihood at each value in grid
likelihood <- dbinom(5, size = 7, prob = p_grid)
likelihood
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# R code 2.4, display the posterior distribution
plot(p_grid, posterior, type = "b", 
     xlab="probability of water", ylab="posterior probabilty")
mtext("3 points")
help(plot)

# 2M2
# R code 2.3
# define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)
p_grid
# define prior
prior <- ifelse(p_grid<0.5, 0, 1)
prior
# compute likelihood at each value in grid
likelihood <- dbinom(5, size = 7, prob = p_grid)
likelihood
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# display the posterior distribution
plot(p_grid, posterior, type = "b", 
     xlab="probability of water", ylab="posterior probabilty")
mtext("3 points")
help("ifelse")

# 2H3
# define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)
p_grid
# define prior
prior <- ifelse(p_grid<0.1, 0.5, 0)
prior
# compute likelihood at each value in grid
likelihood <- dbinom(1, size = 1, prob = p_grid)
likelihood
# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# display the posterior distribution
plot(p_grid, posterior, type = "b", 
     xlab="probability of species A", ylab="posterior probabilty")
mtext("2 points")

# 02/25/2016
# R code 3.1
PrPV <- 0.95 # Pr(vam|positive)
PrPM <- 0.01 # Pr(positive|mortal)
PrV <- 0.001 # Pr(vam)
# the Pr of positive
PrP <- PrPV * PrV + PrPM * (1-PrV)  
# calculate the Pr of correctly identify a vampire
(PrVP <- PrPV*PrV / PrP)

# R code 3.2, compute posteror for the globe tossing model 
# in the begining of this chapter
p_grid <- seq(from=0, to=1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior/sum(posterior)
str(posterior)

# R code 3.3
samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) # replace arg means you put back 
# the sample you picked
str(samples)
samples
PI(samples, prob = 0.8) 
HPDI(samples, prob = 0.8)
help("sample")
# R code 3.4
plot(samples)
# R code 3.5
library(rstan)
library(rethinking)
dens(samples) # plot the density 
help("dens")

# R code 3.6
# add up posterior probability where p<0.5 = the posterior probability 
# that the proportion of water is less than 0.5

sum(posterior[p_grid < 0.5])

# R code 3.7, add up all of the samples below 0.5, and also divide the resulting count
# by the total number of samples
sum(samples<0.5)/1e4
# R code 3.8, the posterior probability lies between 0.5 and 0.75
sum(samples > 0.5 & samples<0.75)/1e4 
samples < 0.5

# R code 3.9, the boundary of the lower 80% posterior probability
quantile(samples, 0.8)
# the boundary of the middle 80% posterior probability lies between 10% and 90% quantile
quantile(samples, c(0.1, 0.9))
help("quantile")
quantile(samples, probs = c(0.8))
help("seq")
seq(0,1,0.25)

#R code 3.11, compute posterior probability and draw 1e4 random samples for 
# observing 3 water out of 3 tosses
p_grid <- seq(0,1, length.out = 1000)
prior <- rep(1,1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior/sum(posterior)
samples<- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

# R code 3.12, the 50% percentile confidence intervel 
PI(samples, prob = 0.5) # don't quite understand this! 
quantile(samples, c(0.25, 0.75))
help(PI) 
# how to check the underlying code for a defined funtion??? 

# practice seq()
seq(0,1,length.out = 10)
help(seq)
seq(from = 0, to = 1, length.out = 10)

# R code 3.13, compute the 50% highest posterior density interval 
HPDI(samples, prob = 0.5)

# R code 3.14, compute the maximum posterior (MAP)
p_grid[which.max(posterior)]
# using samples to approximate the MAP
chainmode(samples, adj=0.01)
help("chainmode")
# R code 3.16, posterior mean or median
mean(samples)
median(samples)

# R code 3.17, suppose p=0.5 is our decision, the expected loss will be 
sum(posterior*abs(0.5 - p_grid))
# R code 3.18, using sapply to repeat calculation, pick every number from p_grid to see 
# which is the best decision. 
loss <- sapply(p_grid, function(d) sum(posterior*abs(d-p_grid))) # pick every p_grid and apply
# some fxn to it, function(d): define the fxn, but this fxn is only used here; the last arg
# applys the function on p_grid, in here it may works like take an item from p_grid, and subtract
# every item in p_grid to see how far this item (d) is away from every item in p_grid. 

help(sapply) # learn more about sapply

# R code 3.19, finding the parameter value that minimize the loss
p_grid[which.min(loss)]

# 03/03/2016
# R code 3.20
dbinom(0:2, size = 2, prob = 0.7)
help("dbinom")
# R code 3.21, a single dummy data observation of w can be sampled with, among two tosses, 
# how many times you can see "w", with the prob of 0.7
rbinom(1, size = 2, prob = 0.7)

#R code 3.22, a set of 10 simulations can be made by 
rbinom(10, size = 2, prob = 0.7)
# R code 3.23, generate 100,000 dummy observations, to verify that each value appears 
# in proportion to its likelihood

dummy_w <- rbinom(1e5, size = 2, prob = 0.7)
table(dummy_w)/1e5
help("table")

# R code 3.24, simulate the same sample as before, 9 tosses
dummy_w <- rbinom(1e5, size = 9, prob = 0.7)
library(rethinking)
simplehist(dummy_w, xlab="dummy water count")

# R code 3.25, to simulate predicted obervations for a single value of p=0.6, and 
# to generate random binomial samples
w <- rbinom(1e4, size = 9, prob = 0.6)
simplehist(w)

# propagate parameter uncertanty into predictions, replace a specific prob with samples
# from posterior
w <- rbinom(1e4, size = 9, prob = samples)
samples
min(samples)
max(samples)
plot(samples)
simplehist(w)

# goal of this analysis
# 1) develop a model that fits the data + prior data: use the likelihood fxn and prior info to model data
# 2) use model to estimate parameters or principles: estimate posterior w/ grid approximation and bayes' equation
#                                                    sample from posterior and ask about probability and max/min parameters
#               why sample instead of using the distribution itself? because it is much easier to estimate from the samples
#               than from the distribution... 
# 3) make predictions and comapre to other situations: use the model and posterior of uncertainty (0.1 to 0.9 figure 3.6 middle)
# to make predictions. 
# Application in real world 
# 1) segregation ratio 
# 2) seed germination 

############ Chapter 4
# R code 4.1
library(rethinking)
pos <- replicate(1000, sum(runif(16, -1, 1))) # suppose each step's length is different
?runif
runif(16, -1, 1)
plot(density(pos))
# 4.2 code
prod(1+runif(12,0,0.1))
# 4.3 code
growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
dens(growth, norm.comp = T)
# R code 4.4
big <- replicate(10000, prod(1 + runif(12,0,0.5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))
dens(big, norm.comp = T)
dens(small, norm.comp = T)
# R code 4.5
log.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))
dens(log.big,norm.comp = T)
dnorm(0,0,0.1)
?dnorm
# 4.6
w <- 6; n <- 9;
p_grid <-seq(0,1, length.out = 100)
posterior <- dbinom(w,n,p_grid)*dunif(p_grid,0,1)
posterior <- posterior/sum(posterior)
# R code 4.7
library(rethinking)
data("Howell1")
d <- Howell1
class(d)
d
# 4.8
str(d)
# 4.9
d$height
# 4.10
d2 <-d[d$age>=18,]
str(d2)
dens(d2$height)
# 4.11
curve(dnorm(x, 178, 20), from = 100, to = 250)
?dnorm
# 4.12
curve(dunif(x, 0, 50), -10, 60)
?dunif # the uniform distribution
# 4.13
sample_mu <- rnorm(1e4, 178, 20)
sample_sigma <- runif(1e4, 0, 50)
prior_h <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_h)
?rnorm
prior_h
# R code 4.14 ??????????????
mu.list <-seq (140, 160, length.out = 200)
sigma.list <-seq(4, 9, length.out = 200)
post <- expand.grid(mu=mu.list, sigma=sigma.list)
?expand.grid # create a data frame from all combinations of factors
post$LL <- sapply(1:nrow(post), function(i) sum( dnorm(
  d2$height, 
  mean=post$mu[i],
  sd = post$sigma[i],
  log = T))) # LL is what? sum of what??? 
?dnorm
?sum
post$prod <- post$LL + dnorm(post$mu, 178, 20, T) +
  dunif(post$sigma, 0, 50, T) # prod is what? 
post$prob <- exp(post$prod - max(post$prod)) # prob is probability
?exp # logarithms and expoenentials
# 4.15
contour_xyz(post$mu, post$sigma, post$prob)
# 4.16
image_xyz(post$mu, post$sigma, post$prob)
?expand.grid
?exp
?"contour_xyz"
# 4.17
sample.row <- sample(1:nrow(post), size = 1e4, replace = T,
                     prob = post$prob)
sample.mu <- post$mu[sample.row]
sample.sigma <- post$sigma[sample.row]
# 4.18
plot(sample.mu, sample.sigma, cex=2, pch=16, col=col.alpha(rangi2,0.1))
# 4.19
dens(sample.mu)
dens(sample.sigma)
# 4.20
HPDI(sample.mu)
HPDI(sample.sigma)
# 4.21, focus on 20 heights
d3 <- sample(d2$height, size = 20)
# 4.22 ???????????????????????
mu.list <-seq (150, 170, length.out = 200)
sigma.list <-seq(4, 20, length.out = 200)
post2 <- expand.grid(mu=mu.list, sigma=sigma.list)
post2$LL <- sapply(1:nrow(post2), function(i) sum( dnorm(
  d3, 
  mean=post2$mu[i],
  sd = post2$sigma[i],
  log = T)))
post2$prod <- post2$LL + dnorm(post2$mu, 178, 20, T) +
  dunif(post2$sigma, 0, 50, T)
post2$prob <- exp(post2$prod - max(post2$prod))

sample2.row <- sample(1:nrow(post2), size = 1e4, replace = T,
                     prob = post2$prob)
sample2.mu <- post2$mu[sample2.row]
sample2.sigma <- post2$sigma[sample2.row]
plot(sample2.mu, sample2.sigma, cex=2, pch=16, col=col.alpha(rangi2,0.1))
# R code 4.23
dens(sample2.sigma, norm.comp = T) # a long tail on the right
# R code 4.24, load data and select out the adults
library(rethinking)
data("Howell1")
d <- Howell1
d2 <- d[d$age >=18,]
# R code 4.25, fitting the model with map
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)
?alist
flist
# R code 4.26
m4.1 <- map(flist, data = d2) 
?map # find max a posterior estimate
# R code 4.27
precis(m4.1)
?precis # display 
# R code 4.28
start <- list(
  mu=mean(d2$height),
  sigma=sd(d2$height)
)
?list # alist VS. list
# R code 4.29, a more informative prior for mu, change sd to 0.1, and build the formula 
# into the call map
m4.2 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data = d2)
precis(m4.2)
# R code 4.30
vcov(m4.1) # variance-covariance matrix ??????????????????
?"vcov" # calculate variance-covariance matrix for a fitted model object
# R code 4.31
diag(vcov(m4.1))
?diag # construct a diagonal matrix
cov2cor(vcov(m4.1))
?cov2cor # scales a covariance matrix into the correpsonding correlation matrix
sqrt(diag(vcov(m4.1)))
precis(m4.1)
# R code 4.32, sample from multi-dimentional posterior
library(rethinking)
post <- extract.samples(m4.1, n = 1e4)
head(post)
mean(post$mu)
mean(post$sigma)
# R code 4.33
precis(post)
plot(post)
# R code 4.34
library(MASS)
post <- mvrnorm(n=1e4, mu = coef(m4.1), Sigma = vcov(m4.1))
?mvrnorm
# R code 4.35
m4.1_logsigma <- map(
  alist(
    height ~ dnorm(mu, exp(log_sigma)),
    mu ~ dnorm(178, 20),
    log_sigma ~ dnorm(2,10)
  ), data = d2)
# R code 4.36
post <- extract.samples(m4.1_logsigma)
sigma <- exp(post$log_sigma)

######## for 03/24/2016 ###########################
library(rethinking)
data("Howell1")

# R code 4.37
plot(d2$height ~ d2$weight)

# R code 4.38
# load data again
d <- Howell1
d2 <- d[d$age >=18, ]

# fit model, question, where is start list???????
m4.3 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(156, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), 
data=d2)
?alist # make a collection of arbitrary R objects. alist does not evaluates the code embeded in
# it. so when define a list of fomulas, should use alist, so the code isn't executed.  

# R code 4.39
m4.3 <- map(
  alist(
    height ~ dnorm(a + b*weight, sigma),
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), 
  data=d2)

# R code 4.40
precis(m4.3)

# R code 4.41
precis(m4.3, corr = T)

# R code 4.42, centering... 
d2$weight.c <- d2$weight - mean(d2$weight)
mean(d2$weight.c)

# R code 4.43
m4.4 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight.c, 
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), 
  data=d2)

# R code 4.44
precis(m4.4, corr = T)

# R code 4.45
plot(height ~ weight, data=d2)
abline(a=coef(m4.3)["a"], b = coef(m4.3)["b"]) ### understand coef... 

# R code 4.46
post <- extract.samples(m4.3)

# R code 4.47
post[1:5,]

# R code 4.48, start with some of the data only, so you will see how adding in more 
# data changes the scatter of the lines, begin with just the first 10 cases in d2
N <- 352
dN <- d2[1:N,] # d <- Howell1; d2 <- d[d$age >=18, ]
mN <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10), 
    sigma ~ dunif(0, 50)
  ), data = dN)
?map # find mode of posterior distribution for arbitrary fixed effet models

# R code 4.49
# extract 20 samples from the posterior
post <- extract.samples(mN, n=20)
?extract.samples

# display raw data and sample size
plot(dN$weight, dN$height,
     xlim = range(d2$weight), ylim = range(d2$height),
     col=rangi2, xlab="weight", ylab="height")
mtext(concat("N= ", N))

# plot the lines, with transparency
for (i in 1:20)
  abline(a=post$a[i], b=post$b[i], col=col.alpha("black", 0.3)) # understand this... 

# R code 4.50
post <- extract.samples(m4.3, n=1e4) # extract 1e4 samples
mu_at_50 <- post$a + post$b *50 
mu_at_50

# R code 4.51
dens(mu_at_50, col=rangi2, lwd=2, xlab="mu|weight=50")

# R code 4.52
HPDI(mu_at_50, prob = 0.89)

# R code 4.53, understand link!!!!!!!!!!!!!!!!! #################
####################### problem start from here##################
precis(m4.3)
mu <- link(m4.3)
str(mu) # problem!!!!!!!!!! doesn't look right

# R code 4.54
# define sequence of weights to compute predictions for 
# these values will be on the horizontal axis
weight.seq <- seq(25, 70, by=1)

# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu <- link(m4.3, data = data.frame(weight=weight.seq))
str(mu)

# R code 4.55
# use type="n" to hide raw data

plot(height ~ weight, d2, type="n")

# loop over samples and plot each mu value
for (i in 1:100)
  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))

# R code 4.56
# summarize the distributon of mu
mu.mean <- apply(mu, 2, mean)
mu.mean
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)

# R code 4.57
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5))

# plot the MAP line, aka the man mu for each weight
lines(weight.seq, mu.mean)

# plot a shaded region for 89% HDPI
shade(mu.HPDI, weight.seq)

# R code 4.58
post <- extract.samples(m4.3)
?extract.samples
mu.link <- function(weight) post$a + post$b*weight
weight.seq <- seq(25, 70, by=1)
mu <- sapply(weight.seq, mu.link)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)

plot(height ~ weight, data=d2, col=col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.HPDI, weight.seq)

# prediction intervals
# R code 4.59
sim.height <- sim(m4.3, data = list(weight=weight.seq)) # m4.3 is the linear model
str(sim.height) 
?sim # difference between R4.59 & R4.58

# R code 4.60
height.PI <- apply(sim.height, 2, PI, prob=0.89)

# R code 4.61, plot
# plot raw data
plot(height~weight, d2, col=col.alpha(rangi2, 0.5))

# draw MAP line
lines(weight.seq, mu.mean)

# draw PI region for simulated heights
shade(height.PI, weight.seq)

# R code 4.62
sim.height <- sim(m4.3, data = list(weight=weight.seq), n = 1e4)
height.PI <- apply(sim.height, 2, PI, prob=0.89)

# plot raw data
plot(height~weight, d2, col=col.alpha(rangi2, 0.5))

# draw MAP line
lines(weight.seq, mu.mean)

# draw PI region for simulated heights
shade(height.PI, weight.seq)

# R code 4.63, how sim works
post <- extract.samples(m4.3)
weight.seq <- 25:70
sim.height <- sapply(weight.seq, function(weight)
  rnorm(
    n=nrow(post),
    mean = post$a+post$b*weight,
    sd=post$sigma))
height.PI <- apply(sim.height, 2, PI, prob=0.89)
?rnorm

############## for 04/04/2016 ###################################
# R code 4.64
library(rethinking)
data("Howell1")
d <- Howell1
str(d)
plot(d$height, d$weight)

# R code 4.65
d$weight.s <- (d$weight - mean(d$weight))/sd(d$weight)
plot(d$height, d$weight.s)

# R code 4.66
d$weight.s2 <- d$weight.s^2
m4.5 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight.s + b2*weight.s2,
    a~ dnorm(178, 100),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10), 
    sigma ~ dunif(0, 50)
  ),
data = d)

# R code 4.67
precis(m4.5)

# R code 4.68
# calculate the mean relationship and the 89% interval 
# of the mean and the prediction
weight.seq <-seq(-2.2, 2, length.out = 30)
pred_dat <- list(weight.s = weight.seq, weight.s2=weight.seq^2)
mu <- link(m4.5, data = pred_dat) 
# take map model fit, sample from posterior distribution, and then compute 
# mu for each case in the data and sample from the posterior distribution
?link
dim(mu)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob=0.89)
sim.height <- sim(m4.5, data = pred_dat)
?sim
sim(m4.5, data = pred_dat)
height.PI <- apply(sim.height, 2, PI, prob=0.89)

# R code 4.69, plot
plot(height ~ weight.s, d, col=col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
shade(height.PI, weight.seq)

# R code 4.70
# fit the model with a slight modification of the parabolic model's code
d$weight.s3 <- d$weight.s^3
m4.6 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight.s +b2 *weight.s2 + b3*weight.s3,
    a ~dnorm(178, 100),
    b1 ~ dnorm(0,10),
    b2 ~ dnorm(0,10),
    b3 ~ dnorm(0,10),
    sigma ~ dunif(0,50)
  ),
data = d)

# R code 4.71
# plot the estimates on the original scale
plot(height ~ weight.s, d, col=col.alpha(rangi2, 0.5), xaxt="n")

# R code 4.72
at <- c(-2, -1, 0, 1,2 )
labels <- at*sd(d$weight) + mean(d$weight)
axis(side = 1, at = at, labels = round(labels, 1))

### notes from R club
# equation R markdown for math symbols
# table function in R make a table from the code 

############### For 04-11-2016 ################################

# R code 5.1
library(rethinking)
data("WaffleDivorce")
d <- WaffleDivorce

# standardize predictor
d$MedianAgeMarriage.s <- (d$MedianAgeMarriage-mean(d$MedianAgeMarriage))/sd(d$MedianAgeMarriage)
# fit model
m5.1 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bA * MedianAgeMarriage.s,
    a ~ dnorm(10, 10),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d)

# R code 5.2 
# compute percentile interval of mean
MAM.seq <- seq(from = -3, to = 3.5, length.out= 30) # why make a list of numbers instead of using the orignal data? 
# because you want to get a precise prediction of the shaded area
mu <- link(m5.1, data = data.frame(MedianAgeMarriage.s=MAM.seq))
mu.PI <- apply(mu, 2, PI)

# plot it all
plot(Divorce ~ MedianAgeMarriage.s, data=d, col=rangi2)
abline(m5.1)
shade(mu.PI, MAM.seq)

############ what if using the original data 
mu.2 <- link(m5.1)
mu.2.PI <- apply(mu.2, 2, PI)
plot(Divorce ~ MedianAgeMarriage.s, data=d, col=rangi2)
abline(m5.1)
shade(mu.2.PI, d$MedianAgeMarriage.s)
#############################################

precis(m5.1) # how to inspect the result, read the result. 

# R code 5.3
d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage) # standardize predictor values
m5.2 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR * Marriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d) # build the model

MAM.seq <- seq(from = -3, to = 3.5, length.out= 30) # make a list of predictor values
mu <- link(m5.2, data = data.frame(Marriage.s=MAM.seq)) # compute model values for each predictor values 
?link
data.frame(Marriage.s=MAM.seq) 
mu
mu.PI <- apply(mu, 2, PI) # compute 95% PI for model values  
mu.PI

# plot it all
plot(Divorce ~ Marriage.s, data=d, col=rangi2) 
# make a plot using data d with Divorce as y axis and Marrige.s as x axis
abline(m5.2) 
?abline
shade(mu.PI, MAM.seq)
?shade

# R code 5.4
m5.3 <- map(
  alist(
    Divorce ~ dnorm(mu, sigma),
    mu <- a + bR*Marriage.s + bA * MedianAgeMarriage.s,
    a ~ dnorm(10, 10),
    bR ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), 
  data = d )
precis(m5.3)

# R code 5.5
plot(precis(m5.3)) # could not plot??? 

# R code 5.6 predictor residual plot
m5.4 <- map(
  alist(
    Marriage.s ~ dnorm(mu, sigma),
    mu <- a + b*MedianAgeMarriage.s,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), 
data = d)

# R code 5.7
# compute the residual by sbutracting the observed marriage rate in each
# state from the predicted rate, based upon using age at marriage
# compuate expected value at MAP, for each state
mu <- coef(m5.4)['a'] + coef(m5.4)['b'] * d$MedianAgeMarriage.s
mu
# compute residual for each state
mu.resid <- d$Marriage.s - mu
d$Marriage.s
mu.resid
# R code 5.8
plot(Marriage.s ~ MedianAgeMarriage.s, d, col=rangi2)
abline(m5.4)
# loop over states
for (i in 1:length(mu.resid)) {
  x <- d$MedianAgeMarriage.s[i]  # x loaction of line segment
  y <- d$Marriage.s[i]  # observed endpoint of line segment
  # draw the line segment
  lines(c(x, x), c(mu[i], y), lwd=0.5, col=col.alpha("black", 0.7))
} 

?lines
# R code 5.9
# prepare new counterfactural data
A.avg <- mean(d$MedianAgeMarriage.s) # mean of standard marriage age
A.avg # a single value
R.seq <- seq(from = -3, to = 3, length.out = 30) # 30 marriage rate values from -3 to 3
R.seq
pred.data <- data.frame( 
  Marriage.s = R.seq,
  MedianAgeMarriage.s=A.avg
) # make a data frame using Marriage.s & MedianAgeMarrige.s
pred.data
# compute counterfactual mean divorce (mu)
mu <- link(m5.3, data = pred.data) 
# compute model values for each combination of predictor value (a constant marriage age)  
###################### need to understand link more thouroughly ########################
head(mu)
dim(mu)
m5.3
mu.mean <- apply(mu, 2, mean) # find mean for mu
mu.PI <- apply(mu, 2, PI) # 95% PI of mu 

# simulate counterfactual divorce outcomes
R.sim <- sim(m5.3, data = pred.data, n = 1e4) # difference between sim() and link() ????????
# simulate 1e4 mu values based on model 5.3 using predictor values of pred.data (constant marriage age)
head(R.sim)
dim(R.sim)
R.PI <- apply(R.sim, 2, PI)

# display predictions, hiding raw data with type="n"
plot(Divorce ~ Marriage.s, data = d, type="n") # what is Marriage.s 
# d$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage) # $Marriage is Marriage rate
mtext("MedianAgeMarriage.s=0")
lines(R.seq, mu.mean) # 
?lines
shade(mu.PI, R.seq) # mu.PI is for the computed mean value?
shade(R.PI, R.seq) # R.PI is for the predicted value? 

# R code 5.10, do the same computation as R code 5.9
# control for Marraige rate
R.avg <- mean(d$Marriage.s)
A.seq <- seq(from = -3, to = 3.5, length.out = 30)
pred.data2 <- data.frame(
  Marriage.s=R.avg,
  MedianAgeMarriage.s=A.seq
)

mu <- link(m5.3, data = pred.data2)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

A.sim <- sim(m5.3, data = pred.data2, n = 1e4)
A.PI <- apply(A.sim, 2, PI)

plot(Divorce ~ MedianAgeMarriage.s, data = d, type="n") 
mtext("Marriage.s=0")
lines(A.seq, mu.mean) 
shade(mu.PI, A.seq) 
shade(A.PI, A.seq) 

# how to understand these two plots? 
# posterior prediciton plots
# R code 5.11
# call link without specifying new data
# so it uses original data
mu <- link(m5.3)

# summarize samples across cases
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# simulate observations
# again no new data, so uses original data
divorce.sim <- sim(m5.3, n = 1e4)
divorce.PI <- apply(divorce.sim, 2, PI) # how these simulated data were used in the plot? 

# R code 5.12 
# plot predictions against observed. add a line to show perfect 
# prediction and line segment for the confidence interval of each prediciton
plot(mu.mean ~ d$Divorce, col=rangi2, ylim=range(mu.PI),
     xlab="Observed divorce", ylab="Predicted divorce") # plot computed data
mu.mean # 50 iterms
mu # 1000 items, computed mu, default is 1000 items
?link
mu.PI # 50 items

abline(a=0, b=1, lty=2) # a the intercept, b the slope
?abline
nrow(d) # 50 items
for(i in 1:nrow(d))
  lines(rep(d$Divorce[i],2), c(mu.PI[1,i], mu.PI[2,i]), 
# for each data point, draw 5% and 95% confididence interval
  col=rangi2)

rep(d$Divorce[1],2)
?rep
mu.PI[1,1]
mu.PI[2,1]
c(mu.PI[1,1], mu.PI[2,1])
mu.PI
?lines

# R code 5.13
identify(x=d$Divorce, y=mu.mean, labels = d$Loc, cex=0.8)

# R code 5.14
# compute residuals
divorce.resid <- d$Divorce - mu.mean
divorce.resid # 50 items
# get ordering by divorce rate
o <- order(divorce.resid) 
?order
# make the plot
dotchart(divorce.resid[o], labels = d$Loc[o], xlim = c(-6,5), cex = 0.6) 
# make a dot chart using divorce resid according to order o 
?dotchart
abline(v =0, col=col.alpha("black", 0.2)) # draw a line at 0 
?abline
for (i in 1:nrow(d)){
  j <- o[i] # which state in order
  lines(d$Divorce[j]-c(mu.PI[1,j], mu.PI[2,j]), rep(i,2)) 
  points(d$Divorce[j]-c(divorce.PI[1,j], divorce.PI[2,j]), rep(i,2), # the divorce.PI used here
         pch=3, cex=0.6, col="red")      
}

o
o[1]
d$Divorce[13]-c(mu.PI[1,13], mu.PI[2,13])
lines(d$Divorce[13]-c(mu.PI[1,13], mu.PI[2,13]))
rep(1,2)

# Note from class
# why standardize... 

################################## For 04/25/2016 #####################################

# R code 5.16
library(rethinking)
data("milk")
d <- milk
str(d)

# R code 5.17 check the simple bivariate regression between kilocalories & neocortex percent
m5.5 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bn*neocortex.perc,
    a ~ dnorm(0, 100),
    bn ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), 
data = d)

# R code. 5.18
d$neocortex.perc # missing data in this column

# R code 5.19 make a new data frame w/ only complete cases in it
dcc <- d[complete.cases(d),]
complete.cases(d)
dcc

# R code 5.20 now check the regression with complete cases
m5.5 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bn*neocortex.perc,
    a ~ dnorm(0, 100),
    bn ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), 
  data = dcc)

# R code 5.21 take a look at the quandratic approximate posterior
precis(m5.5, digits = 3) # the 89% interval of the bn are on both sides of zero, so positive or negative??? 

# R code 5.22 
# a change from the smallest neocortex percent to the largest would result in an unexpected change of 
coef(m5.5)["bn"]*(76-55)
?"coef"

# R code 5.23. plot the predicted mean and 89% interval for the mean to see how the distance on both sides of zero
np.seq <- 0:100 # generate 101 numbers from 0 to 100 with interval of 1 
np.seq
pred.data <- data.frame(neocortex.perc=np.seq) # make a dataframe with the 101 numbers as neocortex percentage
pred.data 

mu <- link(m5.5, data = pred.data, n = 1e4) # compute linear model values for m5.5 using 101 numbers
?link
head(mu)
m5.5
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)
nrow(mu.PI)

plot(kcal.per.g ~ neocortex.perc, data=dcc, col=rangi2)
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1,], lty=2) # the lower 5.5% interval  
lines(np.seq, mu.PI[2,], lty=2) # the upper 5.5% interval 
?lines

# R code 5.24 fit another bivariate regression, with the log of body mass as the predictor variable 
dcc$log.mass <- log(dcc$mass) # log transform mass 
# Q when to transform??? 
min(dcc$log.mass)
max(dcc$log.mass)

# R code 5.25. fit the model
m5.6 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bm * log.mass,
    a ~ dnorm(0, 100),
    bm ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ),
  data = dcc)
precis(m5.6)

# make a plot 
np.seq <- -5:5 # generate 101 numbers from 0 to 100 with interval of 1 
pred.data <- data.frame(log.mass=np.seq) # make a dataframe with the 101 numbers as neocortex percentage

mu <- link(m5.6, data = pred.data, n = 1e4) # compute linear model values for m5.5 using 101 numbers
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ log.mass, data=dcc, col=rangi2)
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1,], lty=2) # the lower 5.5% interval  
lines(np.seq, mu.PI[2,], lty=2) # the upper 5.5% interval 

# 5.26 fit a multivaraite model 
m5.7 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bn*neocortex.perc + bm*log.mass,
    a ~ dnorm(0, 100),
    bn ~ dnorm(0, 1),
    bm ~ dnorm(0, 1),
    sigma ~ dunif(0, 1)
  ), 
  data = dcc)
precis(m5.7)

# R code 5.27
mean.log.mass <- mean(log(dcc$mass)) # only use mean of the log.mass as the log.mass
np.seq <- 0:100
pred.data <- data.frame(
  neocortex.perc = np.seq,
  log.mass=mean.log.mass
)

mu <- link(m5.7, data = pred.data, n = 1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ neocortex.perc, data=dcc, type="n")
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1,], lty=2)
lines(np.seq, mu.PI[2,], lty=2)


mean.neocortex.perc <- mean(dcc$neocortex.perc) # only use mean of the log.mass as the log.mass
np.seq <- -5:5
pred.data <- data.frame(
  neocortex.perc = mean.neocortex.perc,
  log.mass=np.seq
)

mu <- link(m5.7, data = pred.data, n = 1e4)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

plot(kcal.per.g ~ log.mass, data=dcc, type="n")
lines(np.seq, mu.mean)
lines(np.seq, mu.PI[1,], lty=2)
lines(np.seq, mu.PI[2,], lty=2)

# R code 5.28 simulate data in which two meaningful predictors act to mask one another
N <- 100
rho <- 0.7
x_pos <- rnorm(N) # produce 100 numbers with normal distribution, mean of 0 and sd of 1

x_neg <- rnorm(N, mean=rho*x_pos,
               sd=sqrt(1-rho^2)) # produce 100 numbers with normal distribution (mean of )
y <- rnorm(N, mean=x_pos- x_neg)
d <- data.frame(y, x_pos, x_neg)
d

pairs(d)

# R code 5.29 simulate heights and leg lengths of 100 individuals
N <- 100
height <- rnorm(N, 10, 2)
leg_prop <- runif(N, 0.4, 0.5)
leg_left <- leg_prop * height + 
  rnorm(N, 0, 0.02)
leg_right <- leg_prop * height + 
  rnorm(N, 0, 0.02)

d <- data.frame(height, leg_left, leg_right)

# R code 5.30
m5.8 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + bl*leg_left + br*leg_right,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dunif(0, 10)
  ), 
  data=d)
precis(m5.8)

# a graphic view of the precis output
plot(precis(m5.8))

# R code 5.32
post <- extract.samples(m5.8)
?extract.samples
plot(bl ~ br, post, col=col.alpha(rangi2, 0.1), pch=16)

# R code 5.33
sum_blbr <- post$bl + post$br
dens(sum_blbr, col=rangi2, lwd=2, xlab="sum of bl and br")

# R code 5.34
m5.9 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + bl*leg_left,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    sigma ~ dunif(0, 10)
  ), 
  data = d)
precis(m5.9)

# R code 5.35, get back to the primate mild data
data("milk")
d <- milk

# R code 5.36 model kcal.per.g as a fxn of perc.fat & perc.lactose
# with two bivariate regressions
# kcal.per.g regressed on perc.fat
m5.10 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bf * perc.fat,
    a ~ dnorm(0.6, 10),
    bf ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), 
  data = d)

# kcal.per.g regressed on perc.lactose
m5.11 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bl*perc.lactose,
    a ~ dnorm(0.6, 10),
    bl ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), 
  data = d)

precis(m5.10, digits = 3)
precis(m5.11, digits = 3)

# R code 5.37
m5.12 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + bf * perc.fat + bl* perc.lactose,
    a ~ dnorm(0.6, 10),
    bf ~ dnorm(0, 1),
    bl ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ),
  data = d)
precis(m5.12, digits = 3)

# R code 5.38 plot to see the problem ???
pairs(~ kcal.per.g + perc.fat + perc.lactose, # how to understand this fomula
      data=d, col=rangi2)
?pairs

# R code 5.39 compute the correlation between the two variables
cor(d$perc.fat, d$perc.lactose)

# R code 5.40 # how to understand this, leave for later... 
sim.coll <- function(r=0.9){
  d$x <- rnorm(nrow(d), mean=r*d$perc.fat,
  sd=sqrt((1-r^2)*var(d$perc.fat)))
  m <- lm(kcal.per.g ~ perc.fat + x, data = d)
  sqrt(diag(vcov(m)))[2] # stddev of parameter
}

rep.sim.coll <- function(r=0.9, n=100){
  stddev <- replicate(n, sim.coll(r))
  mean(stddev)
}

r.seq <- seq(0, 0.99, by = 0.01)
stddev <- sapply(r.seq, function(z) rep.sim.coll(r=z, n = 100))
plot(stddev ~ r.seq, type="l", col=rangi2, lwd=2, xlab="corrleation")

# R code 5.41 simulate data for post-treatment variable 
# number of plants
N <- 100

# simulate initial heights
h0 <- rnorm(N, 10, 2) # size N, mean 10, sd of 2 
h0 # initial height
hist(h0)

# assign treatments and simulate fungus and growth
treatment <- rep(0:1, each=N/2) # repeat 0 & 1 each N/2 times, 1 trt, 0 no trt
treatment
fungus <- rbinom(N, size = 1, prob = 0.5-treatment*0.4) 
# N plants, each with 1 trial and 0.5-treatment*0.4 probability of fungus infection
fungus # 1 means w/ fungus infection, 0 means w/o
?rbinom
h1 <- h0+ rnorm(N, 5-3*fungus) # final height of plants
set.seed(100)
rnorm(N, 5-3*fungus, 1) # size N, mean 5-3*fungus, sd of 1 yes!!! 
# Q!!! rnomr iterate through every item in 5-3*fungus as the mean? 
set.seed(100)
rnorm(N, 5-3*fungus)
5-3*fungus
h1
?rnorm

# compse a clean data frame
d <- data.frame(h0=h0, h1=h1, treatment=treatment, fungus=fungus)
d

# R code 5.42, fit a model that includes all the available variable from R 5.41
m5.13 <- map(
  alist(
    h1 ~ dnorm(mu, sigma),
    mu <- a + bh*h0 + bt *treatment + bf*fungus, # fomula
    a ~ dnorm(0, 100), # intercept, mean of 0, sd of 100 
    c(bh,bt,bf) ~ dnorm(0, 10), # what is this? QQQQQQ !!!
    sigma ~ dunif(0, 10)
  ),
  data = d)
precis(m5.13)

?dnorm

# R code 5.43 omit post-trt variable fungus
m5.14 <- map(
  alist(
    h1 ~ dnorm(mu, sigma),
    mu <- a + bh*h0 + bt*treatment,
    a ~ dnorm(0, 100),
    c(bh,bt) ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ),
  data = d)
precis(m5.14)

# Note from R club 
# Discussion on plot 5.4 in the book 
# I lost my code again!!! although it is fine this time!
# remember not to checkout head on a branch next time!!! 

##################### for 05/02/2016 ##############

# R code 5.44
library(rethinking)
data(Howell1)
d <- Howell1
str(d)

# R code 5.45, fit a model
m5.15 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + bm*male,
    a ~ dnorm(178, 100),
    bm ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), 
  data = d)
precis(m5.15)
plot(precis(m5.15))

# R code 5.46, the posteiro distribution of male height
post <- extract.samples(m5.15)
mu.male <- post$a + post$bm
PI(mu.male)

# R code 5.47, re-parameterizing the model
m5.15b <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- af * (1-male) + am*male,
    af ~ dnorm(178, 100),
    am ~ dnorm(178, 100),
    sigma ~ dunif(0, 50)
  ), 
  data = d)
precis(m5.15b)
precis(m5.15)

# R code 5.48
data(milk)
d <- milk
unique(d$clade)
?unique

# R code 5.49, to create a dummy variable for the New World Monkey category
d$clade.NWM <- ifelse(d$clade=="New World Monkey", 1, 0)
?ifelse

# R code 5.50, make two more dummy variables
d$clade.OWM <- ifelse(d$clade == "Old World Monkey", 1, 0)
d$clade.S <- ifelse(d$clade=="Strepsirrhine", 1, 0)

d$clade.OWM
d$clade.S

# R code 5.51, fit the model
m5.16 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a + b.NWM*clade.NWM + b.OWM*clade.OWM + b.S*clade.S,
    a ~ dnorm(0.6, 10),
    b.NWM ~ dnorm(0, 1),
    b.OWM ~ dnorm(0, 1),
    b.S ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), 
  data = d)
precis(m5.16)

# R code 5.52, get the posterior distribution of the average milk energy in each category
# sample posterior
post <- extract.samples(m5.16)

# compute averages for each category
mu.ape <- post$a
mu.NWM <- post$a + post$b.NWM
mu.OWN <- post$a + post$b.OWM
mu.S <- post$a + post$b.S

# summarize using precis
precis(data.frame(mu.ape, mu.NWM, mu.OWN, mu.S))

# R code 5.53, differences between the two monkey groups
diff.NWM.OWM <- mu.NWM - mu.OWN
quantile(diff.NWM.OWM, probs=c(0.025, 0.5, 0.975))

# R code 5.54
d$clade_id <- coerce_index(d$clade)

# R code 5.55
m5.16_alt <- map(
  alist(
    kcal.per.g ~ dnorm(mu, sigma),
    mu <- a[clade_id],
    a[clade_id] ~ dnorm(0.6, 10),
    sigma ~ dunif(0, 10)
  ),
  data = d)
precis(m5.16_alt, depth = 2)

# R code 5.62 
data("cars")
glimmer(dist ~ speed, data = cars)
?glimmer

#################### For 05/09/2016 #############
# regulatory prior & information criteria 
# R code 6.1

sppnames <- c("afarensis","africanus","habilis","boisei",
              "rudolfensis","ergaster","sapiens")
brainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)
masskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)
d <- data.frame(species=sppnames, brain=brainvolcc, mass=masskg)
d

# R code 6.2
m6.1 <- lm(brain ~ mass, data = d)

# R code 6.3
1 - var(resid(m6.1))/var(d$brain)
summary(m6.1)

# R code 6.4 
m6.2 <- lm(brain ~ mass + I(mass^2), data = d)
?I

# R code 6.5
m6.3 <- lm(brain ~ mass + I(mass^2) + I(mass^3), data = d)
m6.4 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4), data = d)
m6.4 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5), data = d)
m6.4 <- lm(brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) + I(mass^5) + I(mass^6), data = d)

# R code 6.6
m6.7 <- lm(brain ~ 1, data = d)

# R code 6.7
d.new <- d[-i, ]

library(rethinking)
# R code 6.8 
plot(brain ~ mass, d, col="slateblue")
for (i in 1:nrow(d)){
  d.new <- d[-i, ]
  m0 <- lm(brain ~ mass, d.new)
  abline(m0, col=col.alpha("black", 0.5))
}

# R code 6.9
p <- c(0.7, 0.15, 0.15)
-sum(p*log(p))

# R code 6.10
# fit model with lm
m6.1 <- lm(brain ~ mass, d)

# compute deviance by cheating
(-2) * logLik(m6.1)
? logLik

############## read the below codes laters... ################ 
# R code 6.11 ## try to understand these codes afterwards... 
# standardize the mass before fitting
d$mass.s <- (d$mass- mean(d$mass))/sd(d$mass)
m6.8 <- map(
  alist(
    brain ~ dnorm(mu, sigma),
    mu <- a + b*mass.s
  ), 
  data = d,
  start = list(a=mean(d$brain), b=0, sigma=sd(d$brain)),
  method = "Nelder-Mead")

# extract MAP estimates
theta <- coef(m6.8)

# compute deviance # read these code later... 
dev <- (-2)*sum(dnorm(
  d$brain, 
  mean = theta[1]+theta[2]*d$mass.s,
  sd = theta[3],
  log = TRUE))
dev
?dnorm


# R code 6.12 # read 6.12, 6.13, 6.14 afterwards... 
N <- 20
kseq <- 1:5
dev <- sapply(kseq, function(k){
  print(k);
  r <- replicate(1e4, sim.train.test(N=N, k=k));
  c(mean(r[1,]), mean(r[2,]), sd(r[1,]), sd(r[2,]))
})

# R code 6.13 
r <- mcreplicate(1e4, sim.train.test(N=N, k=k), mc.cores = 4)

# R code 6.14
plot(1:5, dev[1,], ylim=c(min(dev[1:2,])-5), max(dev[1:2,]+10),
    xlim=c(1,5.1), xlab="number of parameters", ylab="deviance",
    pch=16, col=rangi2)

mtext(concat("N=", N))
points((1:5)+0.1, dev[2,])

for (i in kseq){
  pts_in <- dev[1,i]+c(-1,+1)*dev[3,i]
  pts_out <- dev[2,i]+c(-1,+1)*dev[4,i]
  lines(c(i,i), pts_in, col=rangi2)
  line(c(i,i)+0.1, pts_out)
}
?sim.train.test

################ For 05/16/2016 ######################### 
# R code 6.15 WAIC calculation 
library(rethinking)
data(cars)
m <- map(
  alist(
    dist ~ dnorm(mu, sigma),
    mu <- a + b*speed,
    a ~ dnorm(0, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 30)
  ), data = cars)
post <- extract.samples(m, n = 1000)
head(post)
# R code 6.16 need the log-likelihood of each observation i at each sample s from the posterior
n_samples <- 1000
ll <- sapply(1:n_samples, function(s){
 mu <- post$a[s] + post$b[s]*cars$speed
 dnorm(cars$dist, mu, post$sigma[s], log = TRUE)
})
head(cars)
dim(cars)
?dnorm
dim(ll)
head(ll)
nrow(cars)
# R code 6.17
n_cases <- nrow(cars)
lppd <- sapply(1:n_cases, function(i) log_sum_exp(ll[i,]-log(n_samples)))
sum(lppd)
# R code 6.18
pWAIC <- sapply(1:n_cases, function(i) var(ll[i,]))

# R code 6.19 
-2 * (sum(lppd) - sum(pWAIC))

# R code 6.20
waic_vec <- -2*(lppd-pWAIC)
sqrt(n_cases*var(waic_vec))

################# For R club 05/23/2016 ##############################
# R code 6.21 
library(rethinking)

data(milk)
d <- milk[complete.cases(milk),]
d$neocortex <- d$neocortex.perc / 100
dim(d)

# R code 6.22
head(d)
a.start <- mean(d$kcal.per.g) 
a.start
sigma.start <- log(sd(d$kcal.per.g))
sigma.start
?map # find mode of posterior distribution for arbitrary fixed effect models
?alist
?list # 
?dnorm # 
m6.11 <- map(
  alist(
    kcal.per.g ~ dnorm(a, exp(log.sigma)) # I forget how to intepret this... 
  ), 
  data = d, start = list(a=a.start, log.sigma = sigma.start))
m6.12 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)),
    mu <- a + bn*neocortex
  ), 
  data = d, start = list(a=a.start, bn=0, log.sigma = sigma.start))
m6.13 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)),
    mu <- a + bm*log(mass)
  ), data = d, start = list(a=a.start, bm=0, log.sigma=sigma.start))
m6.14 <- map(
  alist(
    kcal.per.g ~ dnorm(mu, exp(log.sigma)),
    mu <- a + bn*neocortex + bm*log(mass)
  ), data = d, start = list(a=a.start, bn=0, bm=0, log.sigma=sigma.start))

# R code 6.23
library(rethinking)
WAIC(m6.14)

# R code 6.24
(milk.models <- compare(m6.11, m6.12, m6.13, m6.14))

# R code 6.25
plot(milk.models, SE=TRUE, dSE=TRUE)

# R code 6.26
diff <- rnorm(1e5, 6.7, 7.26) # generate 1e5 numbers w/ mean of 6.7 and stdv of 7.26
sum(diff<0)/1e5 
?rnorm # generates random deviates 
rnorm(1e5, 6.7, 7.26)
length(diff) 

# R code 6.27
coeftab(m6.11, m6.12, m6.13, m6.14)
?coeftab # returns a table of model coefficients in ros and models in columns

# R code 6.28
plot(coeftab(m6.11, m6.12, m6.13, m6.14)) # something wrong!!! 
?coeftab_plot
# Error in as.double(y) : 
# cannot coerce type 'S4' to vector of type 'double'

# R code 6.29
# compute counterfactual predictions
# neocortex from 0.5 to 0.8
nc.seq <- seq(0.5, 0.8, length.out = 30)
d.predict <- list(
  kcal.per.g = rep(0, 30), # empty outcome
  neocortex = nc.seq, # sequence of neocortex
  mass=rep(4.5, 30) # average mass
)
d.predict 

pred.m6.14 <- link(m6.14, data = d.predict)
?link # compute model values for map samples
dim(pred.m6.14)
head(pred.m6.14)

mu <- apply(pred.m6.14, 2, mean)
mu.PI <- apply(pred.m6.14, 2, PI)
mu
length(mu)
mu.PI

# plot it all
plot(kcal.per.g ~ neocortex, d, col=rangi2)
lines(nc.seq, mu, lty=2)
lines(nc.seq, mu.PI[1,], lty=2) # x and y lengths differ??? 
lines(nc.seq, mu.PI[2,], lty=2)
shade(mu.PI, nc.seq)
mu.PI

# R code 6.30
milk.ensemble <- ensemble(m6.11, m6.12, m6.13, m6.14, data = d.predict)
?ensemble # use link & sim for a list of map model fit to construct Akaike weighted ensemble of predictions
dim(milk.ensemble$link)
head(milk.ensemble$link)

mu <- apply(milk.ensemble$link, 2, mean)
mu.PI < apply(milk.ensemble$link, 2, PI)
lines(nc.seq, mu)
shade(mu.PI, nc.seq)

# markdown file output options: ... cache=TRUE, dependency on the previous chunk of code... 

######################## For 06/06/2016 ######################################

# R code 7.1 
library(rethinking)
data(rugged)
d <- rugged

head(d)
dim(d) #234 countries w/ 51 sets of data 
# make log version of outcome
d$log_gdp <- log(d$rgdppc_2000)
dim(d) #234 countries w/ 52 sets of data 
# extract countries w/ GDP data
dd <- d[complete.cases(d$rgdppc_2000), ]
dim(dd) # 170 countries 

# split countries into Africa & non-Africa 
d.A1 <- dd[dd$cont_africa==1, ] # Africa
dim(d.A1) # 49 countries
d.A0 <- dd[dd$cont_africa==0, ] # not Africa
dim(d.A0) #121 countries 

# R code 7.2 
# African nations
m7.1 <- map(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + bR*rugged,
    a ~ dnorm(8, 100),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d.A1)

# Non African nations
m7.2 <- map(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + bR*rugged,
    a ~ dnorm(8, 100),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = d.A0)

# plot the posterior predictions 
# m7.1
rugged.seq <- seq(0, 8, by=1)
mu <- link(m7.1, data = data.frame(rugged = rugged.seq))
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)

plot(log_gdp ~ rugged, data=d.A1, col=col.alpha(rangi2, 0.5))
lines(rugged.seq, mu.mean)
shade(mu.HPDI, rugged.seq)

# m7.2 
rugged.seq <- seq(0, 8, by=1)
mu <- link(m7.2, data = data.frame(rugged = rugged.seq))
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.89)

plot(log_gdp ~ rugged, data=d.A0, col=col.alpha(rangi2, 0.5))
lines(rugged.seq, mu.mean)
shade(mu.HPDI, rugged.seq)

# R code 7.3 
m7.3 <- map(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + bR*rugged,
    a ~ dnorm(8, 100),
    bR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = dd)

# R code 7.4
m7.4 <- map(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + bR*rugged + bA*cont_africa, # go back to understand dummy variable... 
    a ~ dnorm(8, 100),
    bR ~ dnorm(0, 1),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = dd)

# R code 7.5 
compare(m7.3, m7.4) # understand the result, especially the standard error part... 

# R code 7.6
rugged.seq <- seq(-1, 8, by = 0.25)
length(rugged.seq) # 37 
# compute mu over samples, fixing cont_africa = 0
mu.NotAfrica <- link(m7.4, data = data.frame(cont_africa=0, rugged==rugged.seq))

# compute mu over samples, fixing cont_africa = 1
mu.Africa <- link(m7.4, data = data.frame(cont_africa=1, rugged==rugged.seq))

# summarize to means & intervals 
mu.NotAfrica.mean <- apply(mu.NotAfrica, 2, mean)
length(mu.NotAfrica.mean) # 234, why 234 
mu.NotAfrica.PI <- apply(mu.NotAfrica, 2, PI, prob=0.97)
mu.Africa.mean <- apply(mu.Africa, 2, mean)
length(mu.Africa.mean) # 234 
mu.Africa.PI <- apply(mu.Africa, 2, PI, prob=0.97)

### how to plot figure 7.3 
plot(log_gdp ~ rugged, data=dd, col=col.alpha(rangi2, 0.5))

lines(rugged.seq, mu.Africa.mean)
shade(mu.Africa.PI, rugged.seq)

lines(rugged.seq, mu.NotAfrica.mean)
shade(mu.NotAfrica.PI, rugged.seq)

# R code 7.7 # using all data, add dummy variable, add linear interaction effect 
m7.5 <- map(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + gamma * rugged + bA*cont_africa,
    gamma <- bR + bAR * cont_africa,
    a ~ dnorm(8, 100),
    bA ~ dnorm(0, 1),
    bR ~ dnorm(0, 1),
    bAR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = dd)

# R code 7.8
compare(m7.3, m7.4, m7.5)

# R code 7.9
m7.5b <- map(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + bR*rugged + bAR*rugged*cont_africa + bA*cont_africa,
    a ~ dnorm(8, 100),
    bA ~ dnorm(0, 1),
    bR ~ dnorm(0, 1),
    bAR ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data = dd)

# R code 7.10, calcualte predicted mean and PI using just subset of the data (Africa/NonAfrica)
rugged.seq <- seq(-1, 8, by = 0.25)

mu.Africa <- link(m7.5, data = data.frame(cont_africa=1, rugged= rugged.seq))
mu.Africa.mean <- apply(mu.Africa, 2, mean)
mu.Africa.PI <- apply(mu.Africa, 2, PI, prob= 0.97)

mu.NotAfrica <- link(m7.5, data = data.frame(cont_africa=0, rugged= rugged.seq))
mu.NotAfrica.mean <- apply(mu.NotAfrica, 2, mean)
mu.NotAfrica.PI <- apply(mu.NotAfrica, 2, PI, prob= 0.97)

# R code 7.11 
# plot Afican nations w/ regression, plot using subset of data (Africa/NonAfrica)
d.A1 <- dd[dd$cont_africa==1,]

plot(log(rgdppc_2000) ~ rugged, data=d.A1,
     col=rangi2, ylab="log GDP year 2000",
     xlab="Terrain Ruggedness Index")
mtext("African nations", 3)
lines(rugged.seq, mu.Africa.mean, col=rangi2)
shade(mu.Africa.PI, rugged.seq, col = col.alpha(rangi2, 0.3))

# plot non-Afican nations w/ regression
d.A0 <- dd[dd$cont_africa==0,]

plot(log(rgdppc_2000) ~ rugged, data=d.A0,
     col="black", ylab="log GDP year 2000",
     xlab="Terrain Ruggedness Index")
mtext("Non-African nations", 3)
lines(rugged.seq, mu.NotAfrica.mean, col=rangi2)
shade(mu.NotAfrica.PI, rugged.seq)

# R code 7.12 
precis(m7.5)

# R code 7.13, compute the posterior distribution of gamma
post <- extract.samples(m7.5)
gamma.Africa <- post$bR + post$bAR*1
length(gamma.Africa)
gamma.notAfrica <- post$bR + post$bAR*0

# R code 7.14, get mean of gamma  
mean(gamma.Africa)
mean(gamma.notAfrica)

# R code 7.15, full distribution of the slope within and outside of Africa
dens(gamma.Africa, xlim=c(-0.5, 0.6), ylim=c(0, 5.5),
     xlab="gamma", col=rangi2)
dens(gamma.notAfrica, add = TRUE)

# R code 7.16
diff <- gamma.Africa - gamma.notAfrica
sum(diff<0)/length(diff) 

# R code 7.17
# get min and max rugged values
q.rugged <- range(dd$rugged)
q.rugged
q.rugged[1] 
data.frame(rugged=q.rugged[1], cont_africa=0:1)
# compute lines and confidence intervals
mu.ruggedlo <- link(
  m7.5, data = data.frame(rugged=q.rugged[1], cont_africa=0:1)) # just on 2 predictors? 
head(mu.ruggedlo)
dim(mu.ruggedlo) # 1000 2 
mu.ruggedlo.mean <- apply(mu.ruggedlo, 2, mean)
mu.ruggedlo
head(mu.ruggedlo)
mu.ruggedlo.mean
mu.ruggedlo.PI <- apply(mu.ruggedlo, 2, PI)

mu.ruggedi <- link(m7.5,
                   data = data.frame(rugged=q.rugged[2], cont_africa=0:1))
mu.ruggedi.mean <- apply(mu.ruggedi, 2, mean)
mu.ruggedi.PI <- apply(mu.ruggedi, 2, PI)

# plot it all, splitting points at median 
med.r <- median(dd$rugged) # median 
med.r
ox <- ifelse(dd$rugged > med.r, 0.05, -0.05) # assign values to each country based on their 
# rugged value compared to the median 
?ifelse 
ox
length(ox)
dim(dd)
plot(dd$cont_africa + ox, log(dd$rgdppc_2000), # adding ox to each value differentiate 
# them not only by african or not but also by whether their rugged value is below or above the median 
     col=ifelse(dd$rugged>med.r, rangi2, "black"),
     xlim=c(-0.25, 1.25), xaxt="n", ylab="log GDP year 2000",
     xlab="Continent")
dd$cont_africa + ox
dd$cont_africa
?plot # xaxt 

axis(1, at=c(0,1), labels = c("others", "Africa"))
?axis
lines(0:1, mu.ruggedlo.mean, lty=2)
?lines
mu.ruggedlo.mean
shade(mu.ruggedlo.PI, 0:1)
lines(0:1, mu.ruggedi.mean, col=rangi2)
shade(mu.ruggedi.PI, 0:1, col = col.alpha(rangi2, 0.25))

# R code 7.18
library(rethinking)
data(tulips)
d <- tulips
str(d)

# R code 7.19, build model using two predictors w/ and w/o interaction term
m7.6 <- map(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu <- a + bW*water + bS*shade,
    a ~ dnorm(0, 100),
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ), data = d)

m7.7 <- map(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu <- a + bW*water + bS*shade + bWS*water*shade,
    a ~ dnorm(0, 100),
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    bWS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ), data = d)

# R code 7.20, fix the problem in the last the code 
m7.6 <- map(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu <- a + bW*water + bS*shade,
    a ~ dnorm(0, 100),
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ), 
  data = d,
  method = "Nelder-Mead",
  control=list(maxit=1e4)
  )

m7.7 <- map(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu <- a + bW*water + bS*shade + bWS*water*shade,
    a ~ dnorm(0, 100),
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    bWS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ), 
  data = d,
  method = "Nelder-Mead",
  control=list(maxit=1e4)
  )

# R code 7.21 
coeftab(m7.6, m7.7)
?coeftab # return a table of model coeffecients in row and models in columns

# R code 7.22
compare(m7.6, m7.7)

# R code 7.23, make centered versions of shade and water, just subtract the mean of the original
# from each value
d$shade.c <- d$shade - mean(d$shade)
d$water.c <- d$water - mean(d$water)

range(d$shade)
range(d$shade.c)

# R code 7.24, re-estimate the two regression models, using the new centered variables. 
# add start list, because the very flat priors provide terrible random starting locations. 
m7.8 <- map(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu <- a + bW*water.c + bS*shade.c,
    a ~ dnorm(130, 100), # why change to 130 from 0? 
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ), 
  data = d,
  start = list(a=mean(d$blooms), bS=0, bS=0, sigma=sd(d$blooms)) # start value VS prior, need to understand
  # the contribution of the two 
)

m7.9 <- map(
  alist(
    blooms ~ dnorm(mu, sigma),
    mu <- a + bW*water.c + bS*shade.c + bWS*water.c*shade.c,
    a ~ dnorm(130, 100),
    bW ~ dnorm(0, 100),
    bS ~ dnorm(0, 100),
    bWS ~ dnorm(0, 100),
    sigma ~ dunif(0, 100)
  ), 
  data = d,
  start = list(a=mean(d$blooms), bS=0, bS=0, bWS=0, sigma=sd(d$blooms))
)
coeftab(m7.8, m7.9)

# R code 7.25
k <- coef(m7.7) # uncentered model
k
k[1] + k[2]*2 + k[3]*2 + k[4]*2*2

# R code 7.26
k <- coef(m7.9) # centered model 
k
k[1] + k[2]*0 + k[3]*0 + k[4]*0*0

# R code 7.27
precis(m7.9)

# R code 7.28, plot the implied predictions
# make a plot window with three panels in a single row 
par(mfrow=c(1,3)) # 1 row, 3 columns

# loop over values of water.c and plot predictions
shade.seq <- -1:1
for ( w in -1:1){
  dt <- d[d$water.c==w,]
  plot(blooms ~ shade.c, data=dt, col=rangi2,
       main=paste("water.c=", w), xaxp=c(-1,1,2), ylim=c(0, 362),
       xlab="shade (centered)")
  mu <- link(m7.9, data = data.frame(water.c=w, shade.c=shade.seq))
  mu.mean <- apply(mu, 2, mean)
  mu.PI <- apply(mu, 2, PI, prob= 0.97)
  lines(shade.seq, mu.mean)
  lines(shade.seq, mu.PI[1,], lty=2)
  lines(shade.seq, mu.PI[2,], lty=2)
}

# R code 7.29
m7.x <- lm(y ~ x + z + x*z, data = d)

# R code 7.30
m7.x <- lm(y ~ x*z, data = d)

# R code 7.31, subtract one main effect
m7.x <- lm(y~x + x*z -z, data = d) # can we apply this to our analysis? just consider the interaction effect? 

# R code 7.32, three way interactions model design
m7.x <- lm(y ~ x*z*w, data = d)

# R code 7.33, expand a three-way interaction into a full set of terms
x <- z <- w <- 1
colnames(model.matrix(~x*z*w))
?model.matrix # create a design matrix 

# note from R club
# how to set the prior: plot the actual data, mean as the alpha and stdv from the plot as the stdv, beta as 0 makes more sense. 
# regularizing prior, using broad prior 

# x:z interaction  x*z main effects + interaction 

?try # try an expression allowing error recorvery, can knit always.. "try-error"
library(rethinking)

########################### For 06/13/2016 #####################################
library(rethinking)

# R code 8.1
num_weeks <- 1e5
positions <- rep(0, num_weeks)
current <- 10
for (i in 1:num_weeks){
  # record current position
  positions[i] <- current
  
  # flip coin to generate proposal
  proposal <- current + sample(c(-1,1), size = 1) # sampling to decide clock or counterclockwise
  # now make sure he loops around the archipelago
  if (proposal < 1) proposal <- 10  
  if (proposal > 10) proposal < 1
  
  # move?
  prob_move <- proposal/current # 
  current <- ifelse(runif(1) < prob_move, proposal, current) 
}

runif(1) # pick a number from 0 to 1 randomly w/ a uniform distribution
?ifelse # 

########################## chapter 8 part 2 ###################################

# R code 8.2 
library(rethinking)
data("rugged")
d <- rugged
d$log_gdp <- log(d$rgdppc_2000)
dd <- d[complete.cases(d$rgdppc_2000), ]

# R code 8.3, fit an interaction model with map
m8.1 <- map(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa,
    a ~ dnorm(0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0, 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ), data = dd)
precis(m8.1)
# Mean StdDev  5.5% 94.5%
#   a      9.22   0.14  9.00  9.44
# bR    -0.20   0.08 -0.32 -0.08
# bA    -1.95   0.22 -2.31 -1.59
# bAR    0.39   0.13  0.19  0.60
# sigma  0.93   0.05  0.85  1.01

# R code 8.4 
dd.trim <- dd[, c("log_gdp", "rugged", "cont_africa")]
str(dd.trim)

library('BH')
library('rstan')

# R code 8.5 get samples from posterior distribution using rstan
m8.1stan <- map2stan(
  alist(
    log_gdp ~ dnorm(mu, sigma),
    mu <- a + bR*rugged + bA*cont_africa + bAR*rugged*cont_africa,
    a ~ dnorm(0, 100),
    bR ~ dnorm(0, 10),
    bA ~ dnorm(0, 10),
    bAR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 2)
  ), data = dd.trim) 
?map2stan

# R code 8.6
precis(m8.1stan)

# R code 8.7, run four independent MC for the model above, and to distribute them across 
# seperate processors in your computer 
m8.1stan_4chains <- map2stan(m8.1stan, chains = 4, cores = 4)
precis(m8.1stan_4chains)

# R code 8.8, pull out samples
post <- extract.samples(m8.1stan)
str(post)

# R code 8.9 show correlations between parameters
pairs(post)

# R code 8.10
pairs(m8.1stan)

# R code 8.11
show(m8.1stan) # extract model info include model, DIC, and WAIC 

# R code 8.12 trace plot
plot(m8.1stan)
stancode(m8.1stan) # should print out the stan code for the ruggedness model 

# R code 8.13, something wrong that I could not build the model... 
y <- c(-1,1)
m8.2 <- map2stan(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- alpha
  ), 
  data = list(y=y), start = list(alpha=0, sigma=1),
  chains = 2, iter = 4000, wwarmup = 1000
)

library(rethinking)
library('RcppEigen')

# R code 8.14
precis(m8.2)

# R code 8.15
m8.3 <- map2stan(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- alpha,
    alpha ~ dnorm(1, 10),
    sigma ~ dcauchy(0, 1)
  ), 
  data = list(y=y), start = list(alpha=0, sigma=1),
  chains = 2, iter = 4000, warmup = 1000)
precis(m8.3)

# R code 8.16
y <- rcauchy(1e4, 0, 5)
mu <- sapply(1:length(y), function(i) sum(y[1:1])/i)
plot(mu, type="l")

# R code 8.17 
y <- rnorm(100, mean = 0, sd = 1) 

# R code 8.18
m8.4 <- map2stan(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- a1+a2,
    sigma ~ dcauchy(0, 1)
  ), 
  data = list(y=y), start = list(a1=0, a2=0, sigma=1),
  chains = 2, iter = 4000, warmup = 1000)
precis(m8.4)

# R code 8.19
m8.5 <- map2stan(
  alist(
    y~dnorm(mu, sigma),
    mu <- a1 + a2,
    a1 ~ dnorm(0, 10),
    a2 ~dnorm(0, 10),
    sigma ~ dcauchy(0, 1)
  ), 
  data = list(y=y), start = list(a1=0, a2=0, sigma=1),
  chains = 2, iter = 4000, warmup = 1000)
precis(m8.5)

# get C++ compiler for Rstan installation. 
Sys.setenv(MAKEFLAGS = "-j4") 

fx <- inline::cxxfunction( signature(x = "integer", y = "numeric" ) , '
    return ScalarReal( INTEGER(x)[0] * REAL(y)[0] ) ;
                           ' )
fx( 2L, 5 ) # should be 10

# chapter 9 
# baysien theroeum = prior * likelihood / average likelihood 
# during the previous several chapter, we were assuming that we have normal distribution of our data, but here in this 
# chapter, we are using maximum entropy to get a sense of the likelihood, which is the data distribution. 
# what's the advantage of map2stan compared to ANOVA? it tells you the posterior distribution although doesn't give you
# p value 

# R code 9.1, put each distribution of pebbles in a list 
p <- list() 
p$A <- c(0,0,10,0,0)
p$B <- c(0,1,8,1,0)
p$C <- c(0,2,6,2,0)
p$D <- c(1,2,4,2,1)
p$E <- c(2,2,2,2,2)

# R code 9.2, normalize each distribution so that it is a probability distribution 
p_norm <- lapply(p, function(q) q/sum(q))

# R code 9.3, compute information entropy of each 
(H <- sapply(p_norm, function(q)-sum(ifelse(q==0, 0, q*log(q)))))
# The uncertainty contained in a probability distribution is the average log-probability of an event 

# R code 9.4, the log number of ways per pebble to produce it 
ways <- c(1, 90, 1260, 37800, 113400)
logwayspp <- log(ways)/10
logwayspp




# R code 9.5, build list of the candidate distribution
p <- list()
p[[1]] <- c(1/4, 1/4, 1/4, 1/4)
p[[2]] <- c(2/6, 1/6, 1/6, 2/6)
p[[3]] <- c(1/6, 2/6, 2/6, 1/6)
p[[4]] <- c(1/8, 4/8, 2/8, 1/8)
p

# compute expected value of each 
sapply(p, function(p) sum(p*c(0,1,1,2)))
p

# R code 9.6
# compute entropy of each distribution 
sapply(p, function(p) -sum(p*log(p)))

# change probability of getting blue marble in each draw to be 0.7 
p <- 0.7
(A <- c((1-p)^2, p*(1-p), (1-p)*p, p^2))

# R code 9.8, entropy for the distribution with p equals 0.7 
-sum(A*log(A))

# R code 9.9 
sim.p <- function(G=1.4){
  x123 <- runif(3) # generate 3 uniform random numbers
  x4 <- ((G)*sum(x123)-x123[2]-x123[3])/(2/G) # solve for the relative value of the 4th value 
  z <- sum(c(x123, x4)) 
  p <- c(x123, x4)/z # probability 
  list(H=-sum(p*log(p)), p=p) # compute entropy 
}

# R code 9.10, call the function 1e5 times with expected value of 1.4
H <- replicate(1e5, sim.p(1.4))
dens(as.numeric(H[1,]), adj = 0.1)

# R code 9.11
entropies <- as.numeric(H[1,]) # how was the entropy calculated? 
distribution <- H[2,]

# R code 9.12, get the largest observed entropy 
max(entropies)

# R code 9.12, the distribution with the largest entropy is 
distribution[which.max(entropies)]
?which.max

########################## chapter 10 
# R code 10.1 
library(rethinking)
data("chimpanzees")
d <- chimpanzees

?chimpanzees
# R code 10.2 

head(d)
m10.1 <- map(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a, 
    a ~ dnorm(0, 10)
  ), 
  data = d)
precis(m10.1)

# R code 10.3
logistic(c(0.18, 0.46))

# R code 10.4 
m10.2 <- map(
  alist(
    pulled_left ~ dbinom(1, p), 
    logit(p) <- a + bp*prosoc_left, 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 10)
  ), 
  data = d)

m10.3 <- map(
  alist(
    pulled_left ~ dbinom(1, p), 
    logit(p) <- a + (bp + bpC*condition)*prosoc_left,
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 10), 
    bpC ~ dnorm(0, 10)
  ), 
  data = d)

# R code 10.5 
compare(m10.1, m10.2, m10.3)

# R code 10.6
precis(m10.3)

# R code 10.7 
exp(0.61)

# R code 10.8 
logistic(4)

# R code 10.9 
logistic(4 + 0.61)

# R code 10.10 
# dummy data for predictions across treatments
d.pred <- data.frame(
  prosoc_left = c(0,1,0,1), # right/left/right/left
  condition = c(0,0,1,1) # control/control/partner/partner
)
d.pred
# build prediction ensemble 
chimp.ensemble <- ensemble(m10.1, m10.2, m10.3, data = d.pred)
?ensemble
class(chimp.ensemble)
head(chimp.ensemble$link) # four outcomes for 4 different combinations  
dim(chimp.ensemble$link)
# summarize 
pred.p <- apply(chimp.ensemble$link, 2, mean)
pred.p.PI <- apply(chimp.ensemble$link, 2, PI)

pred.p
pred.p.PI

# R code 10.11
# empty plot frame with good axes
plot(0, 0, type="n", xlab="prosoc_left/condition",
     ylab="proportion pulled left", ylim=c(0,1), xaxt="n",
     xlim=c(1,4))
axis(1, at=1:4, labels = c("0/0", "1/0", "0/1", "1/1"))

# plot raw data, one trend for each of 7 indivividual chimpanzees
# will use by() here; see Overthinking box for explanation
p <- by(d$pulled_left, 
        list(d$prosoc_left, d$condition, d$actor), mean)

by(d$pulled_left, 
   list(d$prosoc_left, d$condition, d$actor), mean)

for(chimp in 1:7)
  lines(1:4, as.vector(p[,,chimp]), col=rangi2, lwd=1.5)

# now superimpose posterior prediction 
lines(1:4, pred.p)
shade(pred.p.PI, 1:4)

# R code 10.12
# clean NAs from the data
d2 <- d
d2$recipient <- NULL

# re-use map fit to get the formula
m10.3stan <- map2stan(m10.3, data = d2, iter = 1e4, warmup = 1000)
precis(m10.3stan)

# R code 10.13
pairs(m10.3stan)

# R code 10.14 
m10.4 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a[actor] + (bp + bpC*condition)*prosoc_left,
    a[actor] ~ dnorm(0, 10),
    bp ~ dnorm(0, 10),
    bpC ~ dnorm(0, 10)
  ), 
  data = d2, chains = 2, iter = 2500, warmup = 500)

# R code 10.15 
unique(d$actor)

# R code 10.16 
precis(m10.4, depth = 2)

# R code 10.17 
post <- extract.samples(m10.4)
str(post) # why I got 4000 rows??? 

# R code 10.18 
dens(post$a[,2])

# R code 10.19 
chimp <- 6 # select actor ID 
d.pred <- list(
  pulled_left = rep(0, 4), # empty outcome
  prosoc_left = c(0,1,0,1), # right/left/right/left
  condition =c(0,0,1,1), # control/control/partner/partner
  actor = rep(chimp, 4)
)
d.pred

link.m10.4 <- link(m10.4, data = d.pred)
pred.p <- apply(link.m10.4, 2, mean)
pred.p.PI <- apply(link.m10.4, 2, PI)

plot(0, 0, type="n", xlab="prosoc_left/condition",
     ylab="proportion pulled left", ylim=c(0, 1), xaxt="n",
     xlim=c(1,4), yaxp=c(0,1,2))
axis(1, at=1:4, labels = c("0/0", "1/0", "0/1", "1/1"))
mtext(paste("actor", chimp))  ##### make good frame for plot 

p <- by(d$pulled_left, 
        list(d$prosoc_left, d$condition, d$actor), mean)

p[,,5]

by(d$pulled_left, 
   list(d$prosoc_left, d$condition, d$actor), mean) # calculate the mean of pulled_left for each 
# combination of values in the three variables prosoc_left, condition, and actor 

lines(1:4, as.vector(p[,,chimp]), col=rangi2, lwd=2)

lines(1:4, pred.p) # add predicted data
shade(pred.p.PI, 1:4) # predicted confidence interval 

# R code 10.20 
data("chimpanzees")
d <- chimpanzees
d.aggregated <- aggregate(d$pulled_left, 
                          list(prosoc_left = d$prosoc_left, condition = d$condition, actor=d$actor),
                          sum)
d.aggregated

# R code 10.21 
m10.5 <- map(
  alist(
    x ~ dbinom(18, p),
    logit(p) <- a + (bp + bp*condition)*prosoc_left, # understand the logit part... 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 10), 
    bpC ~ dnorm(0, 10)
  ), data = d.aggregated)

precis(m10.5)
precis(m10.3) # very different likelihood 

### posterior distribution is the same but with different likelihood. 
post.1 <- extract.samples(m10.5)
str(post.1) # why I got 4000 rows??? 

post.2 <- extract.samples(m10.3)
dens(post.2$a)
#### 
head(post.1)
dim(post.1)
dens(post.1$a)

# R code 10.22 
library(rethinking)
data("UCBadmit")
d <- UCBadmit
head(d)

# R code 10.23 
d$male <- ifelse(d$applicant.gender=="male", 1, 0)
m10.6 <- map(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a + bm*male, 
    a ~ dnorm(0, 10), 
    bm ~ dnorm(0, 10)
  ), data = d)

m10.7 <- map(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a, 
    a ~ dnorm(0, 10)
  ), data = d)

# R code 10.24 
compare(m10.6, m10.7)

# R code 10.25 
precis(m10.6)

# R code 10.26 
post <- extract.samples(m10.6)
p.admit.male <- logistic(post$a + post$bm)
p.admit.female <- logistic(post$a)
diff.admit <- p.admit.male - p.admit.female
quantile(diff.admit, c(0.025, 0.5, 0.975))

dens(diff.admit)

# R code 10.27 
postcheck(m10.6, n=1e4) # plot posterior predictirve check # what the dots are???  
tmp <- postcheck(m10.6, n=1e4)
?postcheck
tmp$mean
plot(tmp$mean)
# draw lines connecting points from same dept
for (i in 1:6){
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]
  y2 <- d$admit[x+1]/d$applications[x+1]
  lines(c(x, x+1), c(y1, y2), col=rangi2, lwd=2)
  text(x+0.5, (y1+y2)/2 + 0.05, d$dept[x], cex=0.8, col=rangi2)
}

# R code 10.28 
# make index 
d$dept_id <- coerce_index(d$dept)
d$dept_id
?coerce_index

# model with unique intercept for each dept
m10.8 <- map(
  alist(
    admit ~dbinom(applications, p), 
    logit(p) <- a[dept_id], 
    a[dept_id] ~ dnorm(0, 10)
  ), data = d)

# model with male differneces as well 
m10.9 <- map(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a[dept_id] + bm*male, 
    a[dept_id] ~ dnorm(0, 10), 
    bm ~ dnorm(0, 10)
  ), data = d)

# R code 10.29 
compare(m10.6, m10.7, m10.8, m10.9)

# R code 10.30 
precis(m10.9, depth = 2)

# R code 10.27 
postcheck(m10.9, n=1e4) # plot posterior predictirve check # what the dots are???  

# draw lines connecting points from same dept
for (i in 1:6){
  x <- 1 + 2*(i-1)
  y1 <- d$admit[x]/d$applications[x]
  y2 <- d$admit[x+1]/d$applications[x+1]
  lines(c(x, x+1), c(y1, y2), col=rangi2, lwd=2)
  text(x+0.5, (y1+y2)/2 + 0.05, d$dept[x], cex=0.8, col=rangi2)
}

# R code 10.31 
m10.9stan <- map2stan(m10.9, chains = 2, iter = 2500, warmup = 500)
precis(m10.9stan, depth = 2)
precis(m10.9, depth = 2)

pairs(m10.9stan) # how to interpret the pairs plot??? 
compare(m10.8, m10.9)
compare(m10.8, m10.9, func = DIC) # doesn't work??? 

# R code 10.32 
m10.7glm <- glm(cbind(admit, reject) ~ 1, data = d, family = binomial)
m10.6glm <- glm(cbind(admit, reject) ~ male, data = d, family = binomial)
m10.8glm <- glm(cbind(admit, reject) ~ dept, data = d, family = binomial)
m10.9glm <- glm(cbind(admit, reject) ~ male + dept, data = d, family = binomial)

?glm

# R code 10.33
data("chimpanzees")
m10.4glm <- glm(pulled_left ~ as.factor(actor) + prosoc_left * condition - condition, 
                data=chimpanzees, family = binomial)

# R code 10.34 
glimmer(pulled_left ~ prosoc_left * condition - condition, 
        data = chimpanzees, family = binomial)
?glimmer # show the map/map2stan formula

# R code 10.35 
# outcome and predictor almost perfectly associated 
y <- c(rep(0, 10), rep(1, 10))
y
x <- c(rep(-1, 9), rep(1, 11))
x
# fit binomial GLM
m.bad <- glm(y ~ x, data = list(y=y, x=x), family = binomial)
list(y=y, x=x)
precis(m.bad)

# R code 10.36 
m.good <- map(
  alist(
    y ~ dbinom(1, p), 
    logit(p) <- a + b*x, 
    c(a, b) ~ dnorm(0, 10)
  ), data = list(y=y, x=x))

precis(m.good) # what's the difference between m.bad & m.good? 

# R code 10.37 
m.good.stan <- map2stan(m.good)
pairs(m.good.stan)

######## for 09/30/2016 ######################################
# R code 10.38 
library(rethinking)
y <- rbinom(1e5, 1000, 1/1000)
c(mean(y), var(y))

# R code 10.39 
library(rethinking)
data("Kline")
d <- Kline
d

# R code 10.40
d$log_pop <- log(d$population)
d$contact_high <- ifelse(d$contact=="high", 1, 0)

# R code 10.41 (don't understand dpois)
m10.10 <- map(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a + bp * log_pop + 
      bc * contact_high + bpc*contact_high*log_pop, 
    a ~ dnorm(0, 100),
    c(bp, bc, bpc) ~ dnorm(0, 1)
  ), 
  data = d)
 
?dpois #the possion distribution 
?dnorm
?map 

# R code 10.42
precis(m10.10, corr = T)
plot(precis(m10.10))

# R code 10.43
post <- extract.samples(m10.10)
lambda_high <- exp(post$a + post$bc + (post$bp + post$bpc)*8) # w/ contact, why multiply by 8??? 
lambda_low <- exp(post$a + post$bp*8) # w/o contact  

head(post)

# R code 10.44
diff <- lambda_high - lambda_low
sum(diff >0)/length(diff)

# R code 10.45
# no interaction 
m10.11 <- map(
  alist(
    total_tools ~ dpois(lambda), # what is this??? 
    log(lambda) <- a + bp*log_pop + bc*contact_high,
    a ~ dnorm(0, 100),
    c(bp,bc) ~ dnorm(0,1)
  ), data = d)

# R code 10.46 
# no contact rate 
m10.12 <- map(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a + bp*log_pop,
    a ~ dnorm(0, 100),
    bp ~ dnorm(0, 1)
  ), data = d)

# no log-population 
m10.13 <- map(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a + bc*contact_high,
    a ~ dnorm(0, 100),
    bc ~ dnorm(0, 1)
  ), data = d)

# R code 10.47 
# intercept only 
m10.14 <- map(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a, 
    a ~ dnorm(0, 100)
  ), data = d)

# compare all using WAIC 
# adding n=1e4 for more stable WAIC estimates
# will also plot the comparison 
(island.compare <- compare(m10.10, m10.11, m10.12, m10.13, m10.14, n=1e4))
plot(island.compare)

# R code 10.48 (need to understand these codes)
# make plot of raw data to begin 
# point character (pch) indicates contact rate 
pch <- ifelse(d$contact_high ==1, 16, 1)
plot(d$log_pop, d$total_tools, col=rangi2, pch=pch, 
     xlab="log-population", ylab="total tools")

# sequence of log-population sizes to compute over 
log_pop.seq <- seq(from=6, to=13, length.out = 30)

# compute trend fro high contact islands
d.pred <- data.frame(
  log_pop = log_pop.seq,
  contact_high = 1
)

lambda.pred.h <- ensemble(m10.10, m10.11, m10.12, data = d.pred)
lambda.med <- apply(lambda.pred.h$link, 2, median)
lambda.PI <- apply(lambda.pred.h$link, 2, PI)

# plot predicted trend for high contact islands
lines(log_pop.seq, lambda.med, col=rangi2)
shade(lambda.PI, log_pop.seq, col = col.alpha(rangi2, 0.2))

# compute trend fro low contact islands 
d.pred <- data.frame(
  log_pop = log_pop.seq,
  contact_high = 0
)

lambda.pred.l <- ensemble(m10.10, m10.11, m10.12, data = d.pred)
lamdda.med <- apply(lambda.pred.l$link, 2, median)
lambda.PI <- apply(lambda.pred.l$link, 2, PI)

# plot again 
lines(log_pop.seq, lamdda.med, lty=2)
shade(lambda.PI, log_pop.seq, col=col.alpha("black", 0.1))

# R code 10.49 
m10.10stan <- map2stan(m10.10, iter = 3000, warmup = 1000, chains = 4)
precis(m10.10stan)
pairs(m10.10stan)

# R code 10.50 
# construct centered predictor 
d$log_pop_c <- d$log_pop - mean(d$log_pop)

# re-estimate
m10.10stan.c <- map2stan(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a + bp*log_pop_c + bc*contact_high + 
      bcp*log_pop_c*contact_high,
    a ~ dnorm(0, 10),
    bp ~ dnorm(0, 1),
    bc ~ dnorm(0, 1),
    bcp ~ dnorm(0, 1)
  ), 
  data = d, iter = 3000, warmup = 1000, chains = 4)

precis(m10.10stan.c)
pairs(m10.10stan.c)

# R code 10.51 
num_days <- 30
y <- rpois(num_days, 1.5)
?rpois # simulate possion distribution 

# R code 10.52
num_weeks <- 4
y_new <- rpois(num_weeks, 0.5*7)

# R code 10.53 
y_all <- c(y, y_new)
exposure <- c(rep(1,30), rep(7,4))
monastery <- c(rep(0,30), rep(1,4))
d <- data.frame(y=y_all, days=exposure, monastery=monastery)
head(d)

# R code 10.54 
# compute the offset 
d$log_days <- log(d$days)

# fit the model 
m10.15 <- map(
  alist(
    y ~ dpois(lambda),
    log(lambda) <- log_days + a + b*monastery,
    a ~ dnorm(0, 100),
    b ~ dnorm(0, 1)
  ), data = d)

# R code 10.55
post <- extract.samples(m10.15)
lambda_old <- exp(post$a)
lambda_new <- exp(post$a + post$b)
precis(data.frame(lambda_old, lambda_new))

############# for 10-28-2016 ###################
# R code 12.1 
library(rethinking)
data("reedfrogs")
d <- reedfrogs
str(d)
head(d)
dens(d$surv)

# R code 12.2
# make the tank cluster variable 
d$tank <- 1:nrow(d)
head(d)
d$density

# fit 
m12.1 <- map2stan(
  alist(
    surv ~ dbinom(density, p),  
    logit(p) <- a_tank[tank],
    a_tank[tank] ~ dnorm(0, 5)  
  ), data = d)

precis(m12.1, depth = 2) 

logistic(0)

# R code 12.3, a multilevel model 
m12.2 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank] ~ dnorm(a, sigma), 
    a ~ dnorm(0, 1), # ??? why normal distribution with these prior? 
    sigma ~ dcauchy(0, 1) # why this cauchy distribution and why these prior? 
  ), data = d, iter = 4000, chains = 4)

?dcauchy
precis(m12.2, depth = 2)
# R code 12.4 
compare(m12.1, m12.2)

# R code 12.5 
# extrac stan samples 
post <- extract.samples(m12.2)
head(post$a_tank)
dim(post$a_tank) # 8000 48 
dim(d) # 48 7 
head(post$a)
head(post$sigma)

# compute median intercept for each tank 
# also transform to probability with logistic 
d$propsurv.est <- logistic(apply(post$a_tank, 2, median)) # posterior median 
# display raw proportional surviving in each tank 
plot(d$propsurv, ylim=c(0,1), pch=16, xaxt="n",
     xlab="tank", ylab="proportion survival", col=rangi2)

head(d)
# overlay posteior median 
points(d$propsurv.est)

# mark posterior median probability across tanks 
abline(h=logistic(median(post$a)), lty=2)

# draw vertical dividers between tank densities 
abline(v=16.5, lwd=0.5)
abline(v=32.5, lwd=0.5)
?abline
text(8, 0, "small tanks")
text(16+8, 0, "medium tanks")
text(32+8, 0, "large tanks")

# R code 12.6 (don't understand this part at all.)
# show first 100 populations in the posterior 
plot(NULL, xlim=c(-3, 4), ylim=c(0, 0.35), 
     xlab="log-odds survive", ylab="Density")
for(i in 1:100)
  curve(dnorm(x, post$a[i], post$sigma[i]), add = T, 
  col=col.alpha("black", 0.2))  

?curve
# sample 8000 imaginary tanks from the posterior distribution 
sim_tanks <- rnorm(8000, post$a, post$sigma)
?rnorm
# transform to probability and visulize 
dens(logistic(sim_tanks), xlab="probability survive")

###################################
# don't use dcauchy for experiment 
################################### 

########## for 11/04/2016 ###########
library(rethinking)
# R code 12.7 
a <- 1.4
sigma <- 1.5
nponds <- 60 
ni <- as.integer(rep(c(1, 10, 25, 35), each=15))

# R code 12.8, simulate the ponds
a_pond <- rnorm(nponds, mean = a, sd = sigma)
a_pond

# R code 1.29 
dsim <- data.frame(pond=1:nponds, ni=ni, true_a=a_pond)
dsim
head(dsim)

dsim$ni

# R code 12.11 # simulate survival numbers 
dsim$si <- rbinom(nponds, prob = logistic(dsim$true_a), size = dsim$ni)
head(dsim)
?rbinom # need to understand rbinom more 

# R code 12.12 
dsim$p_nopool <- dsim$si/dsim$ni # survival rate for each pond 
dsim$ni # number of initial tadpoles 
dsim$pond # pond index 
dsim$true_a # logodds of survival probability for each pond 
dsim$si # survival count

head(dsim)
tail(dsim)

# R code 12.13 
m12.3 <- map2stan(
  alist(
    si ~ dbinom(ni, p), 
    logit(p) <- a_pond[pond], 
    a_pond[pond] ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1)
  ), 
  data = dsim, iter = 1e4, warmup = 1000)

# R code 12.14 
precis(m12.3, depth = 2)

# R code 12.15 
estimated.a_pond <- as.numeric(coef(m12.3)[1:60]) # the coefficient is the predicted value? use this directly? 
dsim$p_partpool <- logistic(estimated.a_pond) 

# R code 12.16 
dsim$p_true <- logistic(dsim$true_a)

# R code 12.17 
nopool_error <- abs(dsim$p_nopool - dsim$p_true)
partpool_error <- abs(dsim$p_partpool-dsim$p_true)

# R code 12.18 
plot(1:60, nopool_error, xlab="pond", ylab="absolute error", col=rangi2, pch=16)
points(1:60, partpool_error)

# R code 12.19 
# code for map2stan, build map w/o compile again, see the book 

################### For December 2nd ########################
library(rethinking)
data("chimpanzees")

# R code 12.20 
set.seed(1)
y1 <- rnorm(1e4, 10, 1)
set.seed(1)
y2 <- 10+ rnorm(1e4, 0, 1)

identical(y1, y2)

# R code 12.21 
d <- chimpanzees
d$recipient <- NULL # get rid of NAs

head(d)

m12.4 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p), 
    logit(p) <- a + a_actor[actor] + (bp + bpC*condition)*prosoc_left,
    a_actor[actor] ~ dnorm(0, sigma_actor), 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 10), 
    bpC ~ dnorm(0, 10), 
    sigma_actor ~ dcauchy(0, 1)
  ), 
  data = d, warmup = 1000, iter = 5000, chains = 4, cores = 3)

plot(m12.4)
precis(m12.4)
par(mfrow=c(1,1))
dens(post$sigma_actor)
 
# R code 12.22 
post <- extract.samples(m12.4)
total_a_actor <- sapply(1:7, function(actor) post$a + post$a_actor[,actor]) # get true varying intercept for each actor 
round(apply(total_a_actor, 2, mean),2 ) # round to 2 digit 

# Two types of clusters 
# R code 12.23 
# prep data 
d$block_id <- d$block # name 'block' is reserved by Stan 

m12.5 <- map2stan(
  alist(
    pulled_left ~ dbinom(1, p), 
    logit(p) <- a + a_actor[actor] + a_block[block_id] + 
      (bp + bpC*condition)*prosoc_left,
    a_actor[actor] ~ dnorm(0, sigma_actor), 
    a_block[block_id] ~ dnorm(0, sigma_block), 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 10), 
    bpC ~ dnorm(0, 10), 
    sigma_actor ~ dcauchy(0, 1), 
    sigma_block ~ dcauchy(0, 1)
  ), 
  data = d, warmup = 1000, iter = 6000, chains = 4, cores = 3)
plot(m12.5)
par(mfrow=c(1,1))
plot(precis(m12.5, depth = 2))

# R code 12.25 
post <- extract.samples(m12.5)
dens(post$sigma_block, xlab="sigma", xlim=c(0,4))
dens(post$sigma_actor, col=rangi2, lwd=2, add = T)
text(2, 0.85, "actor", col = rangi2)
text(0.75, 2, "block")

# R code 12.26 
compare(m12.4, m12.5)

# R code 12.27 
# compute and plot posterior predictions for actor number 2 
chimp <- 2
d.pred <- list(
  prosoc_left = c(0,1,0,1), # right/left/right/left
  condition = c(0,0,1,1), # control/control/partner/partner
  actor=rep(chimp, 4)
)
d.pred
link.m12.4 <- link(m12.4, data = d.pred)
head(link.m12.4) # 4 combinations and 1000 trails 
pred.p <- apply(link.m12.4, 2, mean) # posterior mean
pred.p.PI <- apply(link.m12.4, 2, PI) # 5% confidence interval 

# plot posterior predictions 
plot(0,0, type="n", xlab="prosoc_left/condition", 
     ylab="proportion pulled left", ylim=c(0,1), xaxt="n",
     xlim=c(1,4))
axis(1, at=1:4, labels = c("0/0", "1/0", "0/1", "1/1"))

# plot raw data 
p <- by(d$pulled_left, list(d$prosoc_left, d$condition, d$actor), mean)
for (chimp in 1:7)
lines(1:4, as.vector(p[,,chimp]), col=rangi2, lwd=1.5)

# superimpose posterior predictions 
lines(1:4, pred.p)
shade(pred.p.PI, 1:4)

# R code 12.28 
post <- extract.samples(m12.4)
?extract.samples
str(post)
dim(post$a_actor) #16000 samples and 7 actors 

# R code 12.29 
# plot the density for actor 5 
dens(post$a_actor[,5])

#### R code 12.30 & 12.29 are for posteior prediction w/o link function 
# R code 12.30, construct posterior prediction, explain this code???, make a function to return p (probability to pull left)
p.link <- function(prosoc_left, condition, actor){
  logodds <- with(post, 
                  a + a_actor[,actor] + (bp+bpC*condition)*prosoc_left
  )
  return(logistic(logodds))
}
?with

# R code 12.31 
prosoc_left <- c(0,1,0,1)
condition <- c(0,0,1,1)
pred.raw <- sapply(1:4, function(i) p.link(prosoc_left[i], condition[i],2 )) 
pred.p <- apply(pred.raw, 2, mean)
pred.p.PI <- apply(pred.raw, 2, PI)

# plot to see whether get the same result 

# plot posterior predictions 
plot(0,0, type="n", xlab="prosoc_left/condition", 
     ylab="proportion pulled left", ylim=c(0,1), xaxt="n",
     xlim=c(1,4))
axis(1, at=1:4, labels = c("0/0", "1/0", "0/1", "1/1"))

# plot raw data 
p <- by(d$pulled_left, list(d$prosoc_left, d$condition, d$actor), mean)
for (chimp in 1:7)
  lines(1:4, as.vector(p[,,chimp]), col=rangi2, lwd=1.5)

# superimpose posterior predictions 
lines(1:4, pred.p)
shade(pred.p.PI, 1:4) # exactly the same result compare with using link function 

# R code 12.32 
d.pred <- list(
  prosoc_left = c(0,1,0,1), 
  condtion=c(0,0,1,1),
  actor=rep(2,4)
)
d.pred

# R code 12.33 
# replace varying intercept samples with zeros 
# 1000 samples by 7 actors 
a_actor_zeros <- matrix(0, 1000, 7) # 1000 rows and 7 columns of 0 
?matrix

# R code 12.34 
# fire up link 
# note use of replace list 
link.m12.4 <- link(m12.4, n=1000, data = d.pred,
                   replace=list(a_actor=a_actor_zeros))

# summarize and plot, which actor? new actor?  
pred.p.mean <- apply(link.m12.4, 2, mean)
pred.p.PI <- apply(link.m12.4, 2, PI, prob=0.8)
plot(0,0, type="n", xlab="prosoc_left?condition",
     ylab="proportion pulled left", ylim=c(0,1), xaxt="n",
     xlim=c(1,4))
axis(1,at=1:4, labels = c("0/1","1/0","0/1","1/1"))
lines(1:4, pred.p.mean)
shade(pred.p.PI, 1:4)

# R code 12.35 
# replace varying intercept samples with simulations 
post <- extract.samples(m12.4) 
length(post$sigma_actor)
head(post$sigma_actor)
a_actor_sims <- rnorm(7000, 0, post$sigma_actor) # 7000 samples with mean of 0 and sd of post$sigma_actor

?rnorm
length(a_actor_sims)
a_actor_sims <- matrix(a_actor_sims, 1000, 7) # make this simulated data into matrix 
dim(a_actor_sims)


d.pred
# R code 12.36, pass the simulated intercept into link, predition using simulated data from normal distribution 
link.m12.4 <- link(m12.4, n = 1000, data = d.pred,
                   replace=list(a_actor=a_actor_sims))




# summarize and plot, which actor? simulated new actor from normal distribution?  
pred.p.mean <- apply(link.m12.4, 2, mean)
pred.p.PI <- apply(link.m12.4, 2, PI, prob=0.8)
plot(0,0, type="n", xlab="prosoc_left?condition",
     ylab="proportion pulled left", ylim=c(0,1), xaxt="n",
     xlim=c(1,4))
axis(1,at=1:4, labels = c("0/1","1/0","0/1","1/1"))
lines(1:4, pred.p.mean)
shade(pred.p.PI, 1:4)

# R code 12.37 
post <- extract.samples(m12.4)
# simulate a new actor from the posterior and then compute the probability of pulling the left lever for each of the four trt
sim.actor <- function(i){
  sim_a_actor <- rnorm(1,0, post$sigma_actor[i])
  P <- c(0,1,0,1)
  C <- c(0,0,1,1)
  p <- logistic(
    post$a[i] + 
      sim_a_actor + 
      (post$bp[i] + post$bpC[i]*C)*P
  )
  return(p)
}

length(sim.actor(1)) 
# 12.38 use the function made in 12.37 to simulate 50 actors 
# empty plot 
plot(0,0, type="n", xlab="prosoc_left?condition",
     ylab="proportion pulled left", ylim=c(0,1), xaxt="n",
     xlim=c(1,4))
axis(1,at=1:4, labels = c("0/1","1/0","0/1","1/1"))

# plot 50 simulated actors 
for (i in 1:50){
lines(1:4, sim.actor(i), col=col.alpha("black", 0.5))
}

### over dispersion????? don't understand, think we skipeed that chapter chapter 11 
# R code 12.39 
# prep data 
data(Kline)
d <- Kline
head(d)

d$logpop <- log(d$population)
d$society <- 1:10

# fit model 
m12.6 <- map2stan(
  alist(
    total_tools ~ dpois(mu),
    log(mu) <- a + a_society[society] + bp*logpop, 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 1), 
    a_society[society] ~ dnorm(0, sigma_society),
    sigma_society ~ dcauchy(0, 1)
  ), 
  data = d, iter = 4000, chains = 3
)

plot(m12.6)
precis(m12.6, depth = 2)
par(mfrow=c(1,1))
pairs(m12.6)
WAIC(m12.6)

# R code 12.40 
postcheck(m12.6)
post <- extract.samples(m12.6)
d.pred <- list(
  logpop = seq(from=6, to = 14, length.out = 30),
  society = rep(1,30)
)
a_society_sims <- rnorm(20000, 0, post$sigma_society)
a_society_sims <- matrix(a_society_sims, 2000, 10)

link.m12.6 <- link(m12.6, n = 2000, data = d.pred, 
                   replace=list(a_society=a_society_sims))


# display the raw data and the new prediction envelope 
# plot raw data 
plot(d$logpop, d$total_tools, col=rangi2, pch=16,
     xlab="log population", ylab="total tools")

# plot posteior median 
mu.median <- apply(link.m12.6, 2, median)
lines(d.pred$logpop, mu.median)

# plot 97%, 89%, and 67% intervals (all prime numbers)
mu.PI <- apply(link.m12.6, 2, PI, prob=0.97)
shade(mu.PI, d.pred$logpop)

mu.PI <- apply(link.m12.6, 2, PI, prob=0.89)
shade(mu.PI, d.pred$logpop)

mu.PI <- apply(link.m12.6, 2, PI, prob=0.67)
shade(mu.PI, d.pred$logpop)

###### Note from R club 
# nested classification/hierachy: eg flats within each shelf  
# crossed classification/hierachy: eg colomns and rows in experiment design 

# repeated measure...  

######## For 01-27-2017 ##################### 
library(rethinking)

# R code 13.1 
a <- 3.5 # average morning wait time
b <- (-1) # average difference afternnon wait time
sigma_a <- 1 # std dev in intercepts
sigma_b <- 0.5 # std dev in slopes
rho <- (-0.7) # correlation between intercepts and slopes

# R code 13.2 
Mu <- c(a, b)

# R code 13.3 
cov_ab <- sigma_a*sigma_b*rho
Sigma <- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2)

Sigma

# R code 13.4 
matrix(c(1,2,3,4), nrow = 2, ncol = 2)

# R code 13.5, don't quite understand...############## ???????? 
sigmas <- c(sigma_a, sigma_b) # standard deviations
sigmas
Rho <- matrix(c(1, rho, rho, 1), nrow = 2) # correlation matrix 
Rho

# now matrix multiply to get covarance matrix 
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
diag(sigmas)
Sigma

# R code 13.6 
N_cafes <- 20 # number of cafes 

# R code 13.7 
library(MASS)
set.seed(5) # used to replicate examples
vary_effects <- mvrnorm(N_cafes, Mu, Sigma)
vary_effects

# R code 13.8 
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]

# R code 13.9 
plot(a_cafe, b_cafe, col=rangi2, 
     xlab="intercepts (a_cafe)", ylab="slopes (b_cafe)")
# overlay population distribution 
# install.packages("ellipse")
library("ellipse")
for (i in c(0.1, 0.3, 0.5, 0.8, 0.99))
  lines(ellipse(Sigma, centre = Mu, level=l), col=col.alpha("black", 0.2)) # not work for some reason... and don't understand 

# R code 13.10, simulate robot visit to cafes and collecting data 
N_visits <- 10 # number of visit 
N_visits
afternoon <- rep(0:1, N_visits*N_cafes/2) # robot visit to cafes, 20 in total, 10 visit to each, 1 indicate
# in the afternoon, 0 indicate in the morning. 
afternoon
length(afternoon) # 200 
cafe_id <- rep(1:N_cafes, each=N_visits) # repeat 10 times of each cafe 
cafe_id[1]

mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon # generate average waiting time for each cafe? 
# what this code is doing? 
sigma <- 0.5 # std dev within cafes 
wait <- rnorm(N_visits*N_cafes, mu, sigma) # simulate wait time based using mu and sigma 
d <- data.frame(cafe=cafe_id, afternoon=afternoon, wait=wait) # make this into a dataframe
head(d) 
tail(d)

# R code 13.11
R <- rlkjcorr(1e4, K=2, eta = 2) # 1e4 elements, 2 dimention, and with 2 to control the shape of distribution 
length(R) 
R[,1,2]
dim(R)
class(R) # array in R 
?rlkjcorr # LKJ correlation matrix probability density 
dens(R[,1,2], xlab="correlation") # don't understand this R[,1,2] thing... 

# R code 13.12 
m13.1 <- map2stan(
  alist(
    wait ~ dnorm(mu, sigma), # likelihood
    mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon, # linear model
    c(a_cafe,b_cafe)[cafe] ~ dmvnorm2(c(a,b), sigma_cafe, Rho), # population of varying effects, don't quite
    # understand this code, where is the covariance matrix??? 
    a ~ dnorm(0, 10), 
    b ~ dnorm(0, 10), 
    sigma_cafe ~ dcauchy(0, 2), 
    sigma ~ dcauchy(0, 2), 
    Rho ~ dlkjcorr(2) # prior for covariance 
  ), 
  data = d, 
  iter = 5000, warmup = 2000, chains = 2)

# covariance VS. correlation
stancode(m13.1)
precis(m13.1)

# R code 13.13, examine the posterior correlation between intercept and slopes 
post <- extract.samples(m13.1)
dens(post$Rho[,1,2])

# R code 13.14 
# compute unpooled estimates directly from data 
a1 <- sapply(1:N_cafes, 
             function(i) mean(wait[cafe_id==i & afternoon==0])) # for each cafe, get the mean waiting time 
# in the morning 

b1 <- sapply(1:N_cafes, 
             function(i) mean(wait[cafe_id==i & afternoon==1])) - a1
# get the waiting time for each cafe in the afternoon 

# extract posterior means of partially pooled estimates 
post <- extract.samples(m13.1)
length(post)
class(post)
head(post[[1]])

a2 <- apply(post$a_cafe, 2, mean) # get the mean of intercept 
b2 <- apply(post$b_cafe, 2, mean) # mean of slope 

# plot both and connect with lines 
plot(a1, b1, xlab="intercept", ylab="slope",
     pch=16, col=rangi2, ylim=c(min(b1)-0.1, max(b1)+0.1),
     xlim=c(min(a1)-0.1, max(a1)+0.1)) # make a plot of simulated data intecept and slope? 
points(a2, b2, pch=1) # add posterior intercept and slope 
for (i in 1:N_cafes) lines(c(a1[i], a2[i]), c(b1[i],b2[i])) # connect simulated data and posterior 

# R code 13.15, superimpose the contours of the population 
# compute posterior mean bivariate Gaussian 
Mu_est <- c(mean(post$a), mean(post$b)) 
Mu_est # mu for intercept and slope 
rho_est <- mean(post$Rho[,1,2])
rho_est # what is this ???? 
sa_est <- mean(post$sigma_cafe[,1])
sa_est # mean of stdv of intercept 
sb_est <- mean(post$sigma_cafe[,2])
# mean of stdv of slope 
cov_ab <- sa_est*sb_est*rho_est
# ???? what is this ? 
Sigma_est <- matrix(c(sa_est^2, cov_ab, cov_ab, sb_est^2), ncol = 2)
Sigma_est # correlation matrix???

### rho_est VS sigma_est????????? need to ask...  
# draw contours 
library(ellipse)
for (i in c(0.1, 0.3, 0.5, 0.8, 0.99))
  lines(ellipse(Sigma_est, centre = Mu_est, level = l),
        col=col.alpha("black", 0.2)) # not be able to draw this though... 


# R code 13.16 # on outcome scale 
# convert varying effects to waiting times 
wait_morning_1 <- (a1) # average waiting time in the morning intercept 
wait_afternnon_1 <- (a1+b1) # average waiting time in the afternnon  
wait_morning_2 <- (a2) # waiting time in the morning, partially pooled 
# a for intercept, b for slope, a1,b1 from simulated data, no pooling, a2,b2 from partially pooled data 
wait_afternnon_2 <- (a2+b2) # average waiting time in the afternoon, partially pooled  
length(wait_afternnon_1)
length(wait_afternnon_2)

# R code 13.17 
library(rethinking)
data("UCBadmit")
d <- UCBadmit
d$male <- ifelse(d$applicant.gender=="male", 1,0)
d$dept_id <- coerce_index(d$dept)

# R code 13.18 
m13.2 <- map2stan(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a_dept[dept_id] + bm*male, 
    a_dept[dept_id] ~ dnorm(a, sigma_dept), 
    a ~ dnorm(0, 10), 
    bm ~ dnorm(0,1), 
    sigma_dept ~ dcauchy(0,2)
  ), 
  data = d, warmup = 500, iter = 4500, chains = 3)

precis(m13.2, depth = 2) 

# R code 13.19, fit the model 
m13.3 <- map2stan(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a_dept[dept_id] + 
                bm_dept[dept_id]*male, 
    c(a_dept, bm_dept)[dept_id] ~ dmvnorm2(c(a,bm), sigma_dept, Rho), 
    a ~ dnorm(0, 10), 
    bm ~ dnorm(0,1), 
    sigma_dept ~ dcauchy(0,2),
    Rho ~ dlkjcorr(2)
  ), 
  data = d, warmup = 1000, iter = 5000, chains = 4, cores = 3)

# R code 13.20 
precis(m13.3, depth = 2)
plot(precis(m13.3, par=c("a_dept","bm_dept"), depth = 2)) # doesn't work... 

# R code 13.21, fit the model that ignores gender 
m13.4 <- map2stan(
  alist(
    admit ~ dbinom(applications, p), 
    logit(p) <- a_dept[dept_id], 
    a_dept[dept_id] ~ dnorm(a, sigma_dept), 
    a ~ dnorm(0, 10), 
    sigma_dept ~ dcauchy(0,2)
  ), 
  data = d, warmup = 500, iter = 4500, chains = 3)

precis(m13.4, depth = 2) 
compare(m13.2, m13.3, m13.4)

########## for 02_03_2017 ########################### 
# R code 13.22
library(rethinking)
data("chimpanzees")
d <- chimpanzees
d$recipient <- NULL
d$block_id <- d$block
head(d)

m13.6 <- map2stan(
  alist(
    # likelihood 
    pulled_left ~ dbinom(1, p), # likelihood 
    
    # linear model 
    logit(p) <- A + (BP + BPC*condition)*prosoc_left, # linear model 
    A <- a + a_actor[actor] + a_block[block_id], 
    BP <- bp + bp_actor[actor] + bp_block[block_id],
    BPC <- bpc + bpc_actor[actor] + bpc_block[block_id],
    
    # adaptive priors 
    c(a_actor, bp_actor, bpc_actor)[actor] ~ 
      dmvnorm2(0, sigma_actor, Rho_actor), # covariant matrix 
    c(a_block, bp_block, bpc_block)[block_id] ~ 
      dmvnorm2(0, sigma_block, Rho_block),
      
    # fixed priors 
    c(a,bp,bpc) ~ dnorm(0, 1),
    sigma_actor ~ dcauchy(0, 2), 
    sigma_block ~ dcauchy(0, 2), 
    Rho_actor ~ dlkjcorr(4), 
    Rho_block ~ dlkjcorr(4)
  ), data = d, iter = 5000, warmup = 1000, chains = 3, cores = 3)

# R code 12.23 
m13.6NC <- map2stan(
  alist(
    # likelihood 
    pulled_left ~ dbinom(1, p), # likelihood 
    
    # linear model 
    logit(p) <- A + (BP + BPC*condition)*prosoc_left, # linear model 
    A <- a + a_actor[actor] + a_block[block_id], 
    BP <- bp + bp_actor[actor] + bp_block[block_id],
    BPC <- bpc + bpc_actor[actor] + bpc_block[block_id],
    
    # adaptive priors 
    c(a_actor, bp_actor, bpc_actor)[actor] ~ 
      dmvnormNC(sigma_actor, Rho_actor), # covariant matrix, no 0 here compared the last map2stan model  
    c(a_block, bp_block, bpc_block)[block_id] ~ 
      dmvnormNC(sigma_block, Rho_block),
    
    # fixed priors 
    c(a,bp,bpc) ~ dnorm(0, 1), 
    sigma_actor ~ dcauchy(0, 2), 
    sigma_block ~ dcauchy(0, 2), 
    Rho_actor ~ dlkjcorr(4), 
    Rho_block ~ dlkjcorr(4)
  ), data = d, iter = 5000, warmup = 1000, chains = 3, cores = 3)

precis(m13.6, depth = 2)
precis(m13.6NC, depth = 2)

# R code 13.24 
# extract n_eff values for each model 
neff_c <- precis(m13.6, 2)@output$n_eff # don't understand this code, discuss with Julin  
neff_nc <- precis(m13.6NC,2)@output$n_eff

# plot distributions 
boxplot(list('m13.6'=neff_c, 'm13.6NC'=neff_nc),
        ylab="effective samples", xlab="model")

WAIC(m13.6NC) 
WAIC(m13.6)

# R code 13.25 
precis(m13.6NC, depth = 2, pars = c("sigma_actor", "sigma_block"))

# R code 13.26 
p <- link(m13.6NC)
str(p)

# R code 13.27 
m12.5 <- map2stan(
alist(
  pulled_left ~ dbinom(1, p), 
  logit(p) <- a + a_actor[actor] + a_block[block_id] + 
    (bp + bpC*condition)*prosoc_left,
  a_actor[actor] ~ dnorm(0, sigma_actor), 
  a_block[block_id] ~ dnorm(0, sigma_block), 
  a ~ dnorm(0, 10), 
  bp ~ dnorm(0, 10), 
  bpC ~ dnorm(0, 10), 
  sigma_actor ~ dcauchy(0, 1), 
  sigma_block ~ dcauchy(0, 1)
), 
data = d, warmup = 1000, iter = 6000, chains = 4, cores = 3)

compare(m13.6NC, m12.5)

# note: analogous to our data.... 
# chapter 12: multiple intercept, random effect on the intercept 
# chapter 13 section 1 & 2: multiple slope, random effect on the beta (treatment effect)
# chapter 13 section 3: more than two varying effect, random effect on two varing beta... (two treatments effect on many 
# different genotypes, the effect of each treatment and their interaction are different for each gentype)
# when to use multiple intercept, multiple slopes... 
# when use BRMS to report plot and result, use fitted().... 
# use predict() to report result from lmer & BRMS... 

############## For 02-10-2017 ################ 
# R code 13.29 
# load the distance matrix 
library(rethinking)
data("islandsDistMatrix")

# display short column names, so fits on screen 
Dmat <- islandsDistMatrix
Dmat
colnames(Dmat) <- c("Ml", "Ti", "SC", "Ya", "Fi", "Tr", "Ch", "Mn", "To", "Ha")
round(Dmat, 1)

# R code 13.30 
# linear 
curve(exp(-1*x), from = 0, to = 4, lty=2, 
      xlab = "distance", ylab = "correlation")

?curve # Draws a curve corresponding to a function over the interval [from, to].  
exp(-4)
?exp # the exponential function 
exp(0)
exp(-4)
# squared 
curve(exp(-1*x^2), add = T)

# R code 13.31 
data("Kline2") # load the ordinary data, now with coordicates
d <- Kline2
head(Kline2)
Kline2
d$society <- 1:10 # index observations

m13.7 <- map2stan(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a + g[society] + bp*logpop,
    g[society] ~ GPL2(Dmat, etasq, rhosq, 0.01), # why 0.01, not 1??? 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 1), 
    etasq ~ dcauchy(0, 1), 
    rhosq ~ dcauchy(0, 1)
  ), 
  data = list(
    total_tools = d$total_tools,
    logpop = d$logpop, 
    society = d$society,
    Dmat = islandsDistMatrix), 
warmup = 2000, iter = 1e4, chains = 4)

# R code 13.32 
precis(m13.7, depth = 2)

head(d)
d$society
d$logpop
d$total_tools

library(rethinking)
data("Kline")
d <- Kline
d

m <- map2stan(
  alist(
    total_tools ~ dpois(lambda),
    log(lambda) <- a + bp*logpop, 
    a ~ dnorm(0, 10), 
    bp ~ dnorm(0, 1)  
  ), 
  data = d, warmup = 2000, iter = 4000, chains = 4
)

?dpois #the possion distribution 
?dnorm
?map 

# R code 10.42
precis(m, corr = T)

# R code 13.33 
post <- extract.samples(m13.7)

# plot the posterior median coviance function 
curve(median(post$etasq) * exp(-median(post$rhosq)*x^2), from = 0, to = 10,
      xlab = "distance (thousand km)", ylab = "covariance", ylim=c(0,1),
      yaxp=c(0,1,4), lwd=2) 

# plot 100 functions sampled from posterior 
for(i in 1:100)
  curve(post$etasq[i]*exp(-post$rhosq[i]*x^2), add = T,
        col=col.alpha("black", 0.2))

# R code 13.34 
# compute posterior median covariance among societies 
K <- matrix(0, nrow = 10, ncol = 10) 
K
for (i in 1:10)
  for (j in 1:10)
              exp(-median(post$rhosq) * islandsDistMatrix[i,j]^2) # why all multiply??? 
K
diag(K) <- median(post$etasq) + 0.01 # adding the constant extra coviance beyound eta when i=j  
K 

# convert K to a correlation matrix 
# convert to correlation matrix 
Rho <- round(cov2cor(K), 2) # two digit numbers 

# add row/col names for convenience
colnames(Rho) <- c("Ml", "Ti", "SC", "Ya", "Fi", "Tr", "Ch", "Mn", "To", "Ha")
rownames(Rho) <- colnames(Rho)
Rho # Rho is the corrleation, or distance matrix 

# R code 13.36 
# scale point size to logpop
psize <- d$log_pop / max(d$log_pop)
psize <- exp(psize*1.5)-2
psize

# plot raw data and labels
plot(d$lon2, d$lat, xlab="longitude", ylab="latitute", 
     col=rangi2, cex=psize, pch=16, xlim=c(-50, 30))
# ?plot # cex: point size 

labels <- as.character(d$culture)
# text(d$lon2, d$lat, labels = labels, cex = 0.7)
text(d$lon2, d$lat, labels = labels, cex = 0.7, pos = c(2,4,3,3,4,1,3,2,4,2)) # pos tell it not to overlay 
?text
length(d$lon2)
# overlay lines shaded by Rho 
for (i in 1:10)
  for (j in 1:10)
     if (i < j) # only to compute once 
      lines(c(d$lon2[i],d$lon2[j]), c(d$lat[i], d$lat[j]),
            lwd=2, col=col.alpha("black", Rho[i,j]^2)) 

Rho
?lines 
# understand the code... 

# R code 13.37 
# compute posterior median relationship, ignoring distance 
logpop.seq <-seq(6, 14, length.out = 30)
# post$a # mean intercept 
lambda <- sapply(logpop.seq, function(lp) exp(post$a + post$bp*lp)) # lp, logpop 
lambda
dim(lambda) # 32000, 30 
lambda.median <- apply(lambda, 2, median)
lambda.median
lambda.PI80 <- apply(lambda, 2, PI, prob=0.8)

# plot raw data and labels 
plot(d$logpop, d$total_tools, col=rangi2, cex=psize, pch=16, 
     xlab="log population", ylab="total tools")
text(d$logpop, d$total_tools, labels = labels, cex = 0.7, 
     pos=c(4,3,4,2,2,1,4,4,4,2))

# display posterior predictions 
lines(logpop.seq, lambda.median, lty=2)
lines(logpop.seq, lambda.PI80[1,],lty=2)
lines(logpop.seq, lambda.PI80[2,],lty=2)

# overlay correlations 
for (i in 1:10)
  for (j in 1:10)
    if (i < j)
      lines(c(d$logpop[i], d$logpop[j]),
            c(d$total_tools[i], d$total_tools[j]),
            lwd=2, col=col.alpha("black", Rho[i,j]^2))

# understand the above code, and see how to add lines  

########### for 02-23-2017 ########################## 
# R code 14.1 
# simulate a pancake and return randomly ordered sides (come back to understand the code)
sim_pancake <- function(){
  pancake <- sample(1:3, 1) # sample a pancake from 1 to 3
  sides <- matrix(c(1,1,1,0,0,0),2,3)[,pancake] # defines 3 pancakes, on B/B, one U/U, one B/U 
  sample(sides) # sample 
}

sample(1:3, 1) # take 1 sample from 1:3 
tmp1 <- matrix(c(1,1,1,0,0,0),2,3) # make a matrix with 2 colomns and 3 rows, these are all the sides 
tmp <- matrix(c(1,1,1,0,0,0),2,3)[,2]
sample(tmp) 

# sim 10,000 pancakes 
pancakes <- replicate(1e4, sim_pancake())
up <- pancakes[1,]
down <- pancakes[2,]

# compute proportion 1/1 (BB) out of all 1/1 and 1/0 
num_11_10 <- sum(up==1)
num_11 <- sum(up==1 & down==1)
num_11/num_11_10

# R code 14.2 
library(rethinking)
data("WaffleDivorce")
d <- WaffleDivorce
mean(d$Divorce)
head(d)
d$Divorce.SE
d$Marriage.SE
dim(d)

# points 
plot(d$Divorce ~ d$MedianAgeMarriage, ylim=c(4,15),
     xlab="Median age marriage", ylab="Divorce rate")

# standard errors (don't understand the code)
for (i in 1:nrow(d)){
  ci <- d$Divorce[i] + c(-1,1)*d$Divorce.SE[i] # postive & minus stdv 
  x <- d$MedianAgeMarriage[i]
  lines(c(x,x), ci)
}

# R code 14.3 
dlist <- list(
  div_obs = d$Divorce,
  div_sd = d$Divorce.SE,
  R = d$Marriage,
  A = d$MedianAgeMarriage
) # make a list 
dlist

m14.1 <- map2stan(
  alist(
    div_est ~ dnorm(mu,sigma), # outcome parameter also get a second role as the unknown mean of
    # another distribution, one that "predicts" the observed measurement 
    mu <- a + bA*A + bR*R,
    div_obs ~ dnorm(div_est, div_sd), # the uncertainly in measurement inluences the regression
    # parameters in the linear model, and the regression parameters in the linear model also 
    # influence the uncertainty in the measurement. 
    a ~ dnorm(0, 10),
    bA ~ dnorm(0, 10),
    bR ~ dnorm(0, 10),
    sigma ~ dcauchy(0, 2.5)
  ), 
  data = dlist, 
  start = list(div_est=dlist$div_obs), # start at the observed value for each state 
  WAIC = FALSE, iter = 5000, warmup = 1000, chains = 2, cores = 2, 
  control=list(adapt_delta=0.95) # stan will work harder during warmup and potentially sample more
  # efficiently. 
) ###### crash 

# R code 14.4 
precis(m14.1, depth = 2) # not able to build the model... 

# R code 14.5 
dlist <- list(
  div_obs = d$Divorce,
  div_sd = d$Divorce.SE,
  mar_obs = d$Marriage, 
  mar_sd = d$Marriage.SE,
  A = d$MedianAgeMarriage
)

m14.2 <- map2stan(
  alist(
    div_est ~ dnorm(mu, sigma),
    mu <- a + bA*A + bR*mar_est[i],
    div_obs ~ dnorm(div_est, div_sd),
    mar_ons ~ dnorm(mar_est, mar_sd),
    a ~ dnorm(0, 10),
    bA ~ dnorm(0, 10),
    bR ~ dnorm(0, 10), 
    sigma ~ dcauchy(0, 2.5)
  ), 
  data = dlist, 
  start = list(div_est=dlist$div_obs, mar_est=dlist$mar_obs),
  WAIC = F, iter = 5000, warmup = 1000, chains = 3, cores = 3, 
  control=list(adapt_delta=0.95))






















